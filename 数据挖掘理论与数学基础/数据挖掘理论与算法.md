<div align='center' ><font size='30'>数据挖掘：理论与算法</font></div>





[TOC]



# 一、数据预处理

## 1.数据清洗：
        1)Incomplete
        2)Nosiy
        3)Inconsistent
        4)Redundant
        5)Others(Data type;Imbalanced datasets)
    解决方式：
    	1)Ignore
    	2)Fill in the missing values manully
    	3)Fill in the missing valus atuomatically
    	4)More art than science
## 2.异常值处理、重复数据检测
## 3.类型转换与采样
## 4.数据描述与可视化

1) Normalization 
	1.$v = \frac{v-min}{max-min}$ #01化
	2.$v = \frac{v-\mu}{\sigma}$
	2)均值、中位数、众数、方差、相关系数
	$r_{AB}=\frac{\sum(A-\bar{A}(B-\bar{B})}{(n-1)\sigma_A\sigma_B}=\frac{\sum(AB)-n\bar{AB}}{(n-1)\sigma_A\sigma_B)}$，而pearson's chi-square($x^2$)=$\sum(\frac{(x-\mu)^2}{\mu})$

## 5.特征选择——决策树

一般来说，决策树会产生很多枝条，为了更好的建立模型，我们需要对决策树进行剪枝，常见的剪枝可以分为“预剪枝”（Pre-Pruning）和“后剪枝”（Post-Pruning）。预剪枝是在决策树构造时就进行剪枝。方法是在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。后剪枝就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。判断如何剪枝的指标主要有**纯度**和**信息熵**。

纯度代表指标的分歧程度，假设集合 1：6 次都去打篮球；集合 2：4 次去打篮球，2 次不去打篮球；集合 3：3 次去打篮球，3 次不去打篮球。按照纯度指标来说，集合 1> 集合 2> 集合 3。因为集合 1 的分歧最小，集合 3 的分歧最大。

我们在构造决策树的时候，会基于纯度来构建。而经典的 “不纯度”的指标有三种，分别是信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。

### **信息增益（ID3 算法）：**

​	信息熵：$H(X)=-\sum_{i=1}^{n}p(x_i)log_{b}(x_i)$ 
​    例，假设不抽烟中男占80%，女占20%，X：{a='No-Smoking',b='Smoking'},S为熵，60%不抽烟,40%抽烟，则：
​      	$H(S|X=a)=-0.8log_{2}0.8-0.2log_{2}0.2=0.7219$
​      	$H(S|x=b)=-0.05log_{2}0.05-0.95log_{2}0.95=0.2864$
​     	 $H(S|X)=0.6H(S|X=a)+0.4H(S|X=b)=0.5477$
​    因此，$Gain(S,X)=H(S)-H(S|X)=0.4523$
​    注：Gain为信息增益，即当某个特征被确定后，系统的不确定性下降的值，一般由父节点的信息熵减去子节点的信息熵得到。

​	**但由于该种算法会偏向于枝条更多的树，从而产生Entropy Bias**。所以需要加入一个调节因子，计算新的增益系数，被称为信息增益率，结果如下：

### **信息增益率（C4.5 算法）**

​	$SplitInformation=(S,A) = -\sum_{i=1}^c\frac{\vert {S_i}\vert}{\vert S\vert}log_2\frac{\vert {S_i}\vert}{\vert S\vert}$，所以，$GainRatio(S,A)=\frac{Gain(S,A)}{SplitInformation(S,A)}$

### **基尼指数（Cart 算法）：**

​		Classification And Regression Tree，中文叫做分类回归树，只适用于二叉树。

​	基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。分类的过程本身是一个不确定度降低的过程，即纯度的提升过程。所以 CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。

假设t为节点，则该节点的GINI系数计算公式为：

​	$GINI(t) = 1-\sum \limits_k [p(C_k \vert t)]^2$	,这里 $p(C_k \vert t)$表示节点 t 属于类别 $C_k$ 的概率，节点 t 的基尼系数为 1 减去各类别 $C_k$ 概率平方和。

举例：集合 1：6 个都去打篮球；集合 2：3 个去打篮球，3 个不去打篮球。针对集合 1，所有人都去打篮球，所以 $p(C_k \vert t)$=1，因此 $GINI(t)$=1-1=0。针对集合 2，有一半人去打篮球，而另一半不去打篮球，所以，$p(C_1|t)=0.5$，$p(C_2|t)=0.5$，$GINI(t)=1-（0.5*0.5+0.5*0.5）=0.5$。所以，集合 1 的基尼系数最小，也证明样本最稳定，而集合 2 的样本不稳定性更大。

节点 D 的基尼系数等于子节点 D1 和 D2 的归一化基尼系数之和，用公式表示为：

$GINI(D,A) = \frac{D_1}{D}GINI(D_1)+\frac{D_2}{D}GINI(D_2)$，归一化基尼系数代表的是每个子节点的基尼系数乘以该节点占整体父亲节点 D 中的比例。

因此，所以在属性 A 的划分下，节点 D 的基尼系数为：$GINI(D,A) = \frac{6}{12}GINI(D_1)+\frac{6}{12}GINI(D_2)=0.25$

### **使用 CART 算法来创建分类树**

CART 分类树实际上是基于基尼系数来做属性划分的。在 Python 的 sklearn 中，如果我们想要创建 CART 分类树，可以直接使用 DecisionTreeClassifier 这个类。创建这个类的时候，默认情况下 criterion 这个参数等于 gini，也就是按照基尼系数来选择属性划分，即默认采用的是 CART 分类树。iris 这个数据集，是 sklearn 中也自带的。基于 iris 数据集，构造 CART 分类树的代码如下：

```python
# encoding=utf-8
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
# 准备数据集
iris=load_iris()
# 获取特征集和分类标识
features = iris.data
labels = iris.target
# 随机抽取33%的数据作为测试集，其余为训练集
train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)
# 创建CART分类树
clf = DecisionTreeClassifier(criterion='gini')
# 拟合构造CART分类树
clf = clf.fit(train_features, train_labels)
# 用CART分类树做预测
test_predict = clf.predict(test_features)
# 预测结果与测试集结果作比对
score = accuracy_score(test_labels, test_predict)
print("CART分类树准确率 %.4lf" % score)
```

如果将决策树画出来，那么结果如图所示：

![img](https://static001.geekbang.org/resource/image/c1/40/c1e2f9e4a299789bb6cc23afc6fd3140.png)

首先 train_test_split 可以帮助我们把数据集抽取一部分作为测试集，这样我们就可以得到训练集和测试集。

使用 clf = DecisionTreeClassifier(criterion=‘gini’) 初始化一棵 CART 分类树。这样你就可以对 CART 分类树进行训练。

使用 clf.fit(train_features, train_labels) 函数，将训练集的特征值和分类标识作为参数进行拟合，得到 CART 分类树。

使用 clf.predict(test_features) 函数进行预测，传入测试集的特征值，可以得到测试结果 test_predict。

最后使用 accuracy_score(test_labels, test_predict) 函数，传入测试集的预测结果与实际的结果作为参数，得到准确率 score。

### **使用CART 算法创建回归树**

CART 回归树划分数据集的过程和分类树的过程是一样的，只是回归树得到的预测结果是连续值，而且评判“不纯度”的指标不同。在 CART 分类树中采用的是基尼系数作为标准，那么在 CART 回归树中，要根据样本的混乱程度，也就是样本的离散程度来评价“不纯度”，即样本值与均值差值的绝对数：$\vert x-\mu \vert$，或者方差：$S=\frac{1}{n}\sum(x-\mu)^2$。

上述两种节点划分的标准，分别对应着两种目标函数最优化的标准，即用最小绝对偏差（LAD），或者使用最小二乘偏差（LSD）。一般来说，后者更常见。

```python
# 使用自带的波士顿房价数据
# encoding=utf-8
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_boston
from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error
from sklearn.tree import DecisionTreeRegressor
# 准备数据集
boston=load_boston()
# 探索数据
print(boston.feature_names)
# 获取特征集和房价
features = boston.data
prices = boston.target
# 随机抽取33%的数据作为测试集，其余为训练集
train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=0.33)
# 创建CART回归树
dtr=DecisionTreeRegressor()
# 拟合构造CART回归树
dtr.fit(train_features, train_price)
# 预测测试集中的房价
predict_price = dtr.predict(test_features)
# 测试集的结果评价
print('回归树二乘偏差均值:', mean_squared_error(test_price, predict_price))
print('回归树绝对值偏差均值:', mean_absolute_error(test_price, predict_price)) 
```

具体回归树为：

![img](https://static001.geekbang.org/resource/image/65/61/65a3855aed648b32994b808296a40b61.png)

首先加载了波士顿房价数据集，得到特征集和房价。

然后通过 train_test_split 帮助我们把数据集抽取一部分作为测试集，其余作为训练集。

使用 dtr=DecisionTreeRegressor() 初始化一棵 CART 回归树。

使用 dtr.fit(train_features, train_price) 函数，将训练集的特征值和结果作为参数进行拟合，得到 CART 回归树。

使用 dtr.predict(test_features) 函数进行预测，传入测试集的特征值，可以得到预测结果 predict_price。

最后我们可以求得这棵回归树的二乘偏差均值，以及绝对值偏差均值。

**CART 决策树的剪枝**

CART 决策树的剪枝主要采用的是 CCP 方法，它是一种后剪枝的方法，英文全称叫做 cost-complexity prune，中文叫做代价复杂度。这种剪枝方式用到一个指标叫做节点的表面误差率增益值，以此作为剪枝前后误差的定义。用公式表示则是：
$$
\begin{equation} \alpha = \frac{C(t)-C(T_t)}{\vert T_t -1 \vert}\end{equation} 
$$
其中 Tt 代表以 t 为根节点的子树，C(Tt) 表示节点 t 的子树没被裁剪时子树 Tt 的误差，C(t) 表示节点 t 的子树被剪枝后节点 t 的误差，|Tt|代子树 Tt 的叶子数，剪枝后，T 的叶子数减少为|Tt|-1。

所以节点的表面误差率增益值等于节点 t 的子树被剪枝后的误差变化除以剪掉的叶子数量。因为我们希望剪枝前后误差最小，所以我们要寻找的就是最小α值对应的节点，把它剪掉。这时候生成了第一个子树。重复上面的过程，继续剪枝，直到最后只剩下根节点，即为最后一个子树。

得到了剪枝后的子树集合后，我们需要用验证集对所有子树的误差计算一遍。可以通过计算每个子树的基尼指数或者平方误差，取误差最小的那个树，得到我们想要的结果。

[**此处为利用上述模型跑Titannic](https://github.com/cystanford/Titanic_Data):

**此处为自己用随机树模型跑的Titannic数据的例子**

![img](https://static001.geekbang.org/resource/image/d0/7a/d02e69930c8cf00c93578536933ad07a.png)

![img](https://static001.geekbang.org/resource/image/f0/ea/f09fd3c8b1ce771624b803978f01c9ea.png)

## 6. 贝叶斯

贝叶斯公式（条件概率）：$P(A_i\vert B) = \frac{P(B_i\vert A)*P(A_i)}{\sum \limits_{i=1}^n P(A_i)*P(B \vert A_i)}$

朴素贝叶斯：$w_{map} = arg \quad max \quad P(w_i \vert a_1,a_2,...,a_n)$，$a_1,a_2,...,a_n$表示观测到的属性

​							$= arg \quad max \quad \frac{P(a_1,a_2,...,a_n\vert w_i)*P(w_i)}{P(a_1,a_2,...,a_n)}$

等价于$arg \quad max \quad P(a_1,a_2,...,a_n\vert w_i)*P(w_i)$，这要求各属性之间独立，但这基本不现实，所以现实要求的是：

$arg \quad max \quad P(w_i) \prod \limits_{j}^n P(a_j\vert w_j)$，即各属性之间条件独立。

![img](https://static001.geekbang.org/resource/image/8d/fa/8d16d796670bb4901c7a4c13ca3aa1fa.jpg)

### **朴素贝叶斯分类工作原理**

**离散数据案例**

加入存在以下一组数据，那么请问身高“高”、体重“中”，鞋码“中”的这个人是男还是女？

![img](https://static001.geekbang.org/resource/image/de/5d/de0eb88143721c4503d10f0f7adc685d.png)

假设我们用 A 代表属性，用 A1, A2, A3 分别为身高 = 高、体重 = 中、鞋码 = 中。一共有两个类别，假设用 C 代表类别，那么 C1,C2 分别是：男、女，在未知的情况下我们用 $C_i$ 表示。则上述问题等价于

$P(C_i \vert A_1 A_2 A_3)=\frac{P(A_1A_2A_3\vert C_i)P(C_i)}{P(A_1A_2A_3)}$。 

在这个公式里，因为 $P(A_1A_2A_3)$ 都是固定的，我们想要寻找使得 $P(C_i\vert A_1A_2A_3)$ 的最大值，就等价于求 $P(A_1A_2A_3\vert C_i)P(C_i)$ 最大值。我们假定 Ai 之间是相互独立的，那么：$P(A_1A_2A_3\vert C_i)=P(A_1\vert C_i)P(A_2\vert C_i)P(A_3\vert C_i)$，然后我们需要从 Ai 和 Cj 中计算出 $P(A_j|C_i)$ 的概率，带入到上面的公式得出 $P(A_1A_2A_3\vert C_i)$，最后找到使得 $P(A_1A_2A_3\vert C_i)$ 最大的类别 Cj。

我分别求下这些条件下的概率：P(A1|C1)=1/2, P(A2|C1)=1/2, P(A3|C1)=1/4，P(A1|C2)=0, P(A2|C2)=1/2, P(A3|C2)=1/2，所以 P(A1A2A3|C1)=1/16, P(A1A2A3|C2)=0。因为 P(A1A2A3|C1)P(C1)>P(A1A2A3|C2)P(C2)，所以应该是 C1 类别，即男性。

$A_1$

**连续数据案例**

![img](https://static001.geekbang.org/resource/image/2c/28/2c1a8ae18ec5f6f50455aa54a24ad328.png)

那么如果给定一个新的数据，身高 180、体重 120，鞋码 41，请问该人是男是女呢？

此时，可以假设男性和女性的身高、体重、鞋码都是正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。有了密度函数，就可以把值代入，算出某一点的密度函数的值。

### 朴素贝叶斯分类器工作流程

朴素贝叶斯分类常用于文本分类，尤其是对于英文等语言来说，分类效果很好。它常用于垃圾文本过滤、情感预测、推荐系统等。

![img](https://static001.geekbang.org/resource/image/ac/3f/acd7a8e882bf0205f9b33c43fd61453f.jpg)

**第一阶段：**准备阶段在这个阶段我们需要确定特征属性，比如上面案例中的“身高”、“体重”、“鞋码”等，并对每个特征属性进行适当划分，然后由人工对一部分数据进行分类，形成训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。

**第二阶段：**训练阶段这个阶段就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率。输入是特征属性和训练样本，输出是分类器。

**第三阶段：**应用阶段这个阶段是使用分类器对新数据进行分类。输入是分类器和新数据，输出是新数据的分类结果。好了，在这次课中你了解了概率论中的贝叶斯原理，朴素贝叶斯的工作原理和工作流程，也对朴素贝叶斯的强大和限制有了认识。下一节中，我将带你实战，亲自掌握 Python 中关于朴素贝叶斯分类器工具的使用。

![img](https://static001.geekbang.org/resource/image/de/10/debcda91831caefd356d377ddd1aad10.png)

### 朴素贝叶斯的实践

**高斯朴素贝叶斯：**特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。

**多项式朴素贝叶斯：**特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。

**伯努利朴素贝叶斯：**特征变量是布尔变量，符合 0/1 分布，在文档分类中特征是单词是否出现。

伯努利朴素贝叶斯是以文件为粒度，如果该单词在某文件中出现了即为 1，否则为 0。而多项式朴素贝叶斯是以单词为粒度，会计算在某个文件中的具体次数。而高斯朴素贝叶斯适合处理特征变量是连续变量，且符合正态分布（高斯分布）的情况。比如身高、体重这种自然界的现象就比较适合用高斯朴素贝叶斯来处理。而文本分类是使用多项式朴素贝叶斯或者伯努利朴素贝叶斯。

**TF-IDF的计算**：Term Frequency 和 Inverse Document Frequency即词频和逆向文档频率。

词频 TF 计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数呈正比。

逆向文档频率 IDF，是指一个单词在文档中的区分度。它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。IDF 越大就代表该单词的区分度越大。
$$
TF = \frac{单词出现的次数}{文本单词总数}
$$

$$
IDF = log(\frac{文档总数}{该单词出现的文档总数+1})
$$

$$
TF-IDF = TF*IDF
$$

在 sklearn 中我们直接使用 TfidfVectorizer 类，它可以帮我们计算单词 TF-IDF 向量的值。在这个类中，取 sklearn 计算的对数 log 时，底数是 e，不是 10。

``` python
# 两个构造参数，可以自定义停用词 stop_words 和规律规则 token_pattern。需要注意的是传递的数据结构，停用词 stop_words 是一个列表 List 类型，而过滤规则 token_pattern 是正则表达式。
TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vec = TfidfVectorizer()

documents = [
    'this is the bayes document',
    'this is the second second document',
    'and the third one',
    'is this the document'
]
tfidf_matrix = tfidf_vec.fit_transform(documents)

# 输出所有不重复的值
print('不重复的词:', tfidf_vec.get_feature_names())

# 输出每个词对应的id
print('每个单词的ID:', tfidf_vec.vocabulary_)

# 输出每个词的TF-IDF值
print('每个单词的tfidf值:', tfidf_matrix.toarray())

```

**文档分类**

![img](https://static001.geekbang.org/resource/image/25/c3/257e01f173e8bc78b37b71b2358ff7c3.jpg)

**基于分词的数据准备**，包括分词、单词权重计算、去掉停用词；

### **应用朴素贝叶斯分类进行分类**

先通过训练集得到朴素贝叶斯分类器，然后将分类器应用于测试集，并与实际结果做对比，最终得到测试集的分类准确率。

```python
# 英文
import nltk
word_list = nltk.word_tokenize(text) #分词
nltk.pos_tag(word_list) #标注单词的词性

# 中文
import jieba
word_list = jieba.cut (text) #中文分词

# 加载停用词
stop_words = [line.strip().decode('utf-8') for line in io.open('stop_words.txt').readlines()]

# 计算单词权重，这里 max_df 参数用来描述单词在文档中的最高出现率。假设 max_df=0.5，代表一个单词在 50% 的文档中都出现过了，那么它只携带了非常少的信息，因此就不作为分词统计。
tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)
features = tf.fit_transform(train_contents)

# 应用多项式贝叶斯分类器，我们将特征训练集的特征空间 train_features，以及训练集对应的分类 train_labels 传递给贝叶斯分类器 clf，它会自动生成一个符合特征空间和对应分类的分类器。这里我们采用的是多项式贝叶斯分类器，其中 alpha 为平滑参数。为什么要使用平滑呢？因为如果一个单词在训练样本中没有出现，这个单词的概率就会被计算为 0。但训练集样本只是整体的抽样情况，我们不能因为一个事件没有观察到，就认为整个事件的概率为 0。为了解决这个问题，我们需要做平滑处理。当 alpha=1 时，使用的是 Laplace 平滑。Laplace 平滑就是采用加 1 的方式，来统计没有出现过的单词的概率。这样当训练样本很大的时候，加 1 得到的概率变化可以忽略不计，也同时避免了零概率的问题。当 0<alpha<1 时，使用的是 Lidstone 平滑。对于 Lidstone 平滑来说，alpha 越小，迭代次数越多，精度越高。我们可以设置 alpha 为 0.001。
from sklearn.naive_bayes import MultinomialNB  
clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)

# 测试训练
test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)
test_features=test_tf.fit_transform(test_contents)
# 预测
predicted_labels=clf.predict(test_features)
# 计算准确率
from sklearn import metrics
print metrics.accuracy_score(test_labels, predicted_labels)

```

![img](https://static001.geekbang.org/resource/image/2e/6e/2e2962ddb7e85a71e0cecb9c6d13306e.png)

## 7.SVM(支持向量机)

SVM实质上是将样本投射到高维空间，然后通过构建一个超平面来进行数据类。这个超平面能将不同的样本划分开，同时使得样本集中的点到这个分类超平面的最小距离（即分类间隔）最大化。
$$
超平面的数学表达式可为：g(x)=\omega^T x+b,其中\omega, x \in R^n
所以，距离d_i可表示为：d_i = \frac{\vert \omega x_i+b\vert}{\vert \vert\omega\vert\vert}
$$
以上是线性分类的情况，当需要非线性分类时，需要引进核函数。

SVM 既可以做回归，也可以做分类器。当用 SVM 做回归的时候，我们可以使用 SVR （Support Vector Regression）或 LinearSVR。当做分类器的时候，可以使用的是 SVC （ Support Vector Classification）或者 LinearSVC。

从名字上你能看出 LinearSVC 是个线性分类器，用于处理线性可分的数据，只能使用线性核函数。如果是针对非线性的数据，需要用到 SVC。在 SVC 中，我们既可以使用到线性核函数（进行线性划分），也能使用高维的核函数（进行非线性划分）。

我们首先使用 SVC 的构造函数：model = svm.SVC(kernel=‘rbf’, C=1.0, gamma=‘auto’)，这里有三个重要的参数 kernel、C 和 gamma。kernel 代表核函数的选择，它有四种选择，只不过默认是 rbf，即高斯核函数。

**linear：线性核函数**，在数据线性可分的情况下使用的，运算速度快，效果好。不足在于它不能处理线性不可分的数据。

**poly：多项式核函数**，可以将数据从低维空间映射到高维空间，但参数比较多，计算量大。

**rbf：高斯核函数（默认）**，可以将样本映射到高维空间，但相比于多项式核函数来说所需的参数比较少，通常性能不错，所以是默认使用的核函数。

**sigmoid：sigmoid 核函数**，sigmoid 经常用在神经网络的映射中。因此当选用 sigmoid 核函数时，SVM 实现的是多层神经网络。

**参数 C** 代表目标函数的惩罚系数，惩罚系数指的是分错样本时的惩罚程度，默认情况下为 1.0。当 C 越大的时候，分类器的准确性越高，但同样容错率会越低，泛化能力会变差。相反，C 越小，泛化能力越强，但是准确性会降低。

**参数 gamma** 代表核函数的系数，默认为样本特征数的倒数，即 gamma = 1 / n_features。

### **在 sklearn 中使用 SVM**

 [以美国威斯康辛的乳腺癌数据为例](https://github.com/cystanford/breast_cancer_data/)

```python
# 加载数据集，你需要把数据放到目录中
data = pd.read_csv("./data.csv")
# 数据探索
# 因为数据集中列比较多，我们需要把dataframe中的列全部显示出来
pd.set_option('display.max_columns', None)
print(data.columns)
print(data.head(5))
print(data.describe())

# 将特征字段分成3组
features_mean= list(data.columns[2:12])
features_se= list(data.columns[12:22])
features_worst=list(data.columns[22:32])
# 数据清洗
# ID列没有用，删除该列
data.drop("id",axis=1,inplace=True)
# 将B良性替换为0，M恶性替换为1
data['diagnosis']=data['diagnosis'].map({'M':1,'B':0})

# 将肿瘤诊断结果可视化
sns.countplot(data['diagnosis'],label="Count")
plt.show()
# 用热力图呈现features_mean字段之间的相关性
corr = data[features_mean].corr()
plt.figure(figsize=(14,14))
# annot=True显示每个方格的数据
sns.heatmap(corr, annot=True)
plt.show()

# 特征选择
features_remain = ['radius_mean','texture_mean', 'smoothness_mean','compactness_mean','symmetry_mean', 'fractal_dimension_mean'] 

# 抽取30%的数据作为测试集，其余作为训练集
train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test
# 抽取特征选择的数值作为训练和测试数据
train_X = train[features_remain]
train_y=train['diagnosis']
test_X= test[features_remain]
test_y =test['diagnosis']

# 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1
ss = StandardScaler()
train_X = ss.fit_transform(train_X)
test_X = ss.transform(test_X)

# 创建SVM分类器
model = svm.SVC()
# 用训练集做训练
model.fit(train_X,train_y)
# 用测试集做预测
prediction=model.predict(test_X)
print('准确率: ', metrics.accuracy_score(test_y,prediction))
```

![img](https://static001.geekbang.org/resource/image/79/82/797fe646ae4668139600fca2c50c5282.png)

## 8.KNN算法和K-means算法

**相同：**都需要计算距离，而距离的主要计算方法有如下几种：

​	欧氏距离：$d=\sqrt{(x_1-y1)^2+(x_2-y_2)^2}$，同理可以推广的n维。

​	曼哈顿距离：$d=\vert x_1-y_1\vert +\vert x_2-y_2 \vert$

​	切比雪夫距离：$d=\sqrt[p]{\sum \limits_{i=1}^n \vert x_i-y_i\vert^p}$，不是一个距离，而是一组距离的定义。对于 n 维空间中的两个点$ x(x_1,x_2,…,x_n)$ 和$ y(y_1,y_2,…,y_n)$

​	余弦距离：$max(\vert x_1-y_1\vert,\vert x_2-y_2 \vert)$，即取两者中最大值。

**区别：**

1.KNN算法是分类算法，分类算法首先需要有学习语料，然后通过学习语料的学习之后的模板来匹配测试语料集，将测试语料集合进行按照预先学习的语料模板来分类，又称为有监督学习。

​	1）计算待分类物体与其他物体之间的距离；

​	2）统计距离最近的 K 个邻居；

​	3）对于 K 个最近的邻居，它们属于哪个分类最多，待分类物体就属于哪一类。

其中K 值应该是个实践出来的结果，并不是我们事先而定的。在工程上，一般采用交叉验证的方式选取 K 值。注：这里的K值指的是k个邻近的单位。

2.K-means算法是聚类算法，聚类算法与分类算法最大的区别是聚类算法没有学习语料集合，又称为无监督学习。这个算法的本质是确定 K 类的中心点，但该算法需要人为确定K的值，这个K指的是K种类别，而且这个K在实际中非常难以确定。

​	1）选取 K 个点作为初始的类中心点，这些点一般都是从数据集中随机抽取的；

​	2）将每个点分配到最近的类中心点，这样就形成了 K 个类，然后重新计算每个类的中心点；

​	3）重复第二步，直到类不发生变化，或者你也可以设置最大迭代次数，这样即使类中心点发生变化，但是只要达到最大迭代次数就会结束，或者可以设置迭代的变化差，当变化差值在很小范围内时就停止迭代。

### **KNN算法在sklearn中的实践：**

```python
# KNN
from sklearn.neighbors import KNeighborsClassifier #分类
from sklearn.neighbors import KNeighborsRegressor #回归

# 加载数据
digits = load_digits()
data = digits.data
# 数据探索
print(data.shape)
# 查看第一幅图像
print(digits.images[0])
# 第一幅图像代表的数字含义
print(digits.target[0])
# 将第一幅图像显示出来
plt.gray()
plt.imshow(digits.images[0])
plt.show()


# 分割数据，将25%的数据作为测试集，其余作为训练集（你也可以指定其他比例的数据作为训练集）
train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)
# 采用Z-Score规范化
ss = preprocessing.StandardScaler()
train_ss_x = ss.fit_transform(train_x)
test_ss_x = ss.transform(test_x)


# 创建KNN分类器
knn = KNeighborsClassifier() 
# 注：使用构造函数 KNeighborsClassifier(n_neighbors=5, weights=‘uniform’, algorithm=‘auto’, leaf_size=30)，这里有几个比较主要的参数，.n_neighbors：即 KNN 中的 K 值，代表的是邻居的数量，一般设定为5；weights：是用来确定邻居的权重；algorithm：用来规定计算邻居的方法；leaf_size：代表构造 KD 树或球树时的叶子数，默认是 30，调整 leaf_size 会影响到树的构造和搜索速度。
knn.fit(train_ss_x, train_y) 
predict_y = knn.predict(test_ss_x) 
print("KNN准确率: %.4lf" % accuracy_score(test_y, predict_y))

```

![img](https://static001.geekbang.org/resource/image/d0/e1/d08f489c3bffaacb6910f32a0fa600e1.png)

### **K-means在算法中的实践：**

```python

from sklearn.cluster import KMeans
# 注：K-Means 在内，sklearn.cluster 一共提供了 9 种聚类方法，比如 Mean-shift，DBSCAN，Spectral clustering（谱聚类）等。

KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')
# n_clusters: 即 K 值；max_iter： 最大迭代次数，如果聚类很难收敛的话，设置最大迭代次数可以及时得到反馈结果；n_init：初始化中心点的运算次数，默认是 10。程序是否能快速收敛和中心点的选择关系非常大，所以需要多次选择中心点选择，来争取整体时间上的快速收敛还是非常值得的；init： 即初始值选择的方式，默认是采用优化过的 k-means++ 方式，你也可以自己指定中心点，或者采用 random 完全随机的方式，一般推荐采用优化过的 k-means++ 方式；algorithm：k-means 的实现算法，有“auto” “full”“elkan”三种。一般来说建议直接用默认的"auto"。简单说下这三个取值的区别，如果你选择"full"采用的是传统的 K-Means 算法，“auto”会根据数据的特点自动选择是选择“full”还是“elkan”。我们一般选择默认的取值，即“auto” 。

# coding: utf-8
from sklearn.cluster import KMeans
from sklearn import preprocessing
import pandas as pd
import numpy as np
# 输入数据
data = pd.read_csv('data.csv', encoding='gbk')
train_x = data[["2019年国际排名","2018世界杯","2015亚洲杯"]]
df = pd.DataFrame(train_x)
kmeans = KMeans(n_clusters=3)
# 规范化到[0,1]空间
min_max_scaler=preprocessing.MinMaxScaler()
train_x=min_max_scaler.fit_transform(train_x)
# kmeans算法
kmeans.fit(train_x)
predict_y = kmeans.predict(train_x)
# 合并聚类结果，插入到原数据中
result = pd.concat((data,pd.DataFrame(predict_y)),axis=1)
result.rename({0:u'聚类'},axis=1,inplace=True)
print(result)
```

![img](https://static001.geekbang.org/resource/image/eb/c5/eb60546c6a3d9bc6a1538049c26723c5.png)

## **9.EM聚类**

EM 的英文是 Expectation Maximization，所以 EM 算法也叫最大期望算法

我们先看一个简单的场景：假设你炒了一份菜，想要把它平均分到两个碟子里，该怎么分？很少有人用称对菜进行称重，再计算一半的分量进行平分。大部分人的方法是先分一部分到碟子 A 中，然后再把剩余的分到碟子 B 中，再来观察碟子 A 和 B 里的菜是否一样多，哪个多就匀一些到少的那个碟子里，然后再观察碟子 A 和 B 里的是否一样多……整个过程一直重复下去，直到份量不发生变化为止。你能从这个例子中看到三个主要的步骤：初始化参数、观察预期、重新估计。首先是先给每个碟子初始化一些菜量，然后再观察预期，这两个步骤实际上就是期望步骤（Expectation）。如果结果存在偏差就需要重新估计参数，这个就是最大化步骤（Maximization）。这两个步骤加起来也就是 EM 算法的过程。

![img](https://static001.geekbang.org/resource/image/91/3c/91f617ac484a7de011108ae99bd8cb3c.jpg)

### **工作原理**

EM 算法是一种求解最大似然估计的方法，通过观测样本，来找出样本的模型参数。即通过结果的可能性最大来倒推参数值。

EM 算法最直接的应用就是求参数估计。如果我们把潜在类别当做隐藏变量，样本看做观察值，就可以把聚类问题转化为参数估计问题。这也就是 EM 聚类的原理。相比于 K-Means 算法，EM 聚类更加灵活，比如下面这两种情况，K-Means 会得到下面的聚类结果。

![img](https://static001.geekbang.org/resource/image/ba/ca/bafc98deb68400100fde69a41ebc66ca.jpg)

因为 K-Means 是通过距离来区分样本之间的差别的，且每个样本在计算的时候只能属于一个分类，称之为是硬聚类算法。而 EM 聚类在求解的过程中，实际上每个样本都有一定的概率和每个聚类相关，叫做软聚类算法。

你可以把 EM 算法理解成为是一个框架，在这个框架中可以采用不同的模型来用 EM 进行求解。常用的 EM 聚类有 GMM 高斯混合模型和 HMM 隐马尔科夫模型。GMM（高斯混合模型）聚类就是 EM 聚类的一种。比如上面这两个图，可以采用 GMM 来进行聚类。和 K-Means 一样，我们事先知道聚类的个数，但是不知道每个样本分别属于哪一类。通

常，我们可以假设样本是符合高斯分布的（也就是正态分布）。每个高斯分布都属于这个模型的组成部分（component），要分成 K 类就相当于是 K 个组成部分。这样我们可以先初始化每个组成部分的高斯分布的参数，然后再看来每个样本是属于哪个组成部分。这也就是 E 步骤。再通过得到的这些隐含变量结果，反过来求每个组成部分高斯分布的参数，即 M 步骤。

反复 EM 步骤，直到每个组成部分的高斯分布参数不变为止。这样也就相当于将样本按照 GMM 模型进行了 EM 聚类。

### **总结**

EM 算法相当于一个框架，你可以采用不同的模型来进行聚类，比如 GMM（高斯混合模型），或者 HMM（隐马尔科夫模型）来进行聚类。GMM 是通过概率密度来进行聚类，聚成的类符合高斯分布（正态分布）。而 HMM 用到了马尔可夫过程，在这个过程中，我们通过状态转移矩阵来计算状态转移的概率。HMM 在自然语言处理和语音识别领域中有广泛的应用。

在 EM 这个框架中，E 步骤相当于是通过初始化的参数来估计隐含变量。M 步骤就是通过隐含变量反推来优化参数。最后通过 EM 步骤的迭代得到模型参数。 EM 算法是一个不断观察和调整的过程。通过求硬币正面概率的例子，你可以理解如何通过初始化参数来求隐含数据的过程，以及再通过求得的隐含数据来优化参数。通过上面 GMM 图像聚类的例子，你可以知道很多 K-Means 解决不了的问题，EM 聚类是可以解决的。在 EM 框架中，我们将潜在类别当做隐藏变量，样本看做观察值，把聚类问题转化为参数估计问题，最终把样本进行聚类

![img](https://static001.geekbang.org/resource/image/d8/80/d839e80d911add15add41163fa03ee80.png)

### **EM聚类算法在sklearn的实践**

在 sklearn 中创建 GMM 聚类。首先我们使用 gmm = GaussianMixture(n_components=1, covariance_type=‘full’, max_iter=100) 来创建 GMM 聚类，其中有几个比较主要的参数：

1.n_components：即高斯混合模型的个数，也就是我们要聚类的个数，默认值为 1。如果你不指定 n_components，最终的聚类结果都会为同一个值。	

2.covariance_type：代表协方差类型。一个高斯混合模型的分布是由均值向量和协方差矩阵决定的，所以协方差的类型也代表了不同的高斯混合模型的特征。协方差类型有 4 种取值：

covariance_type=full，代表完全协方差，也就是元素都不为 0；

covariance_type=tied，代表相同的完全协方差；

covariance_type=diag，代表对角协方差，也就是对角不为 0，其余为 0；

covariance_type=spherical，代表球面协方差，非对角为 0，对角完全相同，呈现球面的特性。

3.max_iter：代表最大迭代次数，EM 算法是由 E 步和 M 步迭代求得最终的模型参数，这里可以指定最大迭代次数，默认值为 100。创建完 GMM 聚类器之后，我们就可以传入数据让它进行迭代拟合。我们使用 fit 函数，传入样本特征矩阵，模型会自动生成聚类器，然后使用 prediction=gmm.predict(data) 来对数据进行聚类，传入你想进行聚类的数据，可以得到聚类结果 prediction。

你能看出来拟合训练和预测可以传入相同的特征矩阵，这是因为聚类是无监督学习，你不需要事先指定聚类的结果，也无法基于先验的结果经验来进行学习。只要在训练过程中传入特征值矩阵，机器就会按照特征值矩阵生成聚类器，然后就可以使用这个聚类器进行聚类了。

**案例：用 EM 算法对王者荣耀数据进行聚类**

源数据：

![img](https://static001.geekbang.org/resource/image/3c/a0/3c4e14e7b33fc211f96fe0108f6196a0.png)

这里我们收集了 69 名英雄的 20 个特征属性，这些属性分别是最大生命、生命成长、初始生命、最大法力、法力成长、初始法力、最高物攻、物攻成长、初始物攻、最大物防、物防成长、初始物防、最大每 5 秒回血、每 5 秒回血成长、初始每 5 秒回血、最大每 5 秒回蓝、每 5 秒回蓝成长、初始每 5 秒回蓝、最大攻速和攻击范围等。具体的数据集你可以在 GitHub 上下载：https://github.com/cystanford/EM_data。

1、首先加载数据源；

2、在准备阶段，我们需要对数据进行探索，包括采用数据可视化技术，让我们对英雄属性以及这些属性之间的关系理解更加深刻，然后对数据质量进行评估，是否进行数据清洗，最后进行特征选择方便后续的聚类算法；

3、聚类阶段：选择适合的聚类模型，这里我们采用 GMM 高斯混合模型进行聚类，并输出聚类结果，对结果进行分析。

```python
from sklearn.mixture import GaussianMixture

# -*- coding: utf-8 -*-
import pandas as pd
import csv
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
 
# 数据加载，避免中文乱码问题
data_ori = pd.read_csv('./heros7.csv', encoding = 'gb18030')
features = [u'最大生命',u'生命成长',u'初始生命',u'最大法力', u'法力成长',u'初始法力',u'最高物攻',u'物攻成长',u'初始物攻',u'最大物防',u'物防成长',u'初始物防', u'最大每5秒回血', u'每5秒回血成长', u'初始每5秒回血', u'最大每5秒回蓝', u'每5秒回蓝成长', u'初始每5秒回蓝', u'最大攻速', u'攻击范围']
data = data_ori[features]
 
# 对英雄属性之间的关系进行可视化分析
# 设置plt正确显示中文
plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签
plt.rcParams['axes.unicode_minus']=False #用来正常显示负号
# 用热力图呈现features_mean字段之间的相关性
corr = data[features].corr()
plt.figure(figsize=(14,14))
# annot=True显示每个方格的数据
sns.heatmap(corr, annot=True)
plt.show()
 
# 相关性大的属性保留一个，因此可以对属性进行降维
features_remain = [u'最大生命', u'初始生命', u'最大法力', u'最高物攻', u'初始物攻', u'最大物防', u'初始物防', u'最大每5秒回血', u'最大每5秒回蓝', u'初始每5秒回蓝', u'最大攻速', u'攻击范围']
data = data_ori[features_remain]
data[u'最大攻速'] = data[u'最大攻速'].apply(lambda x: float(x.strip('%'))/100)
data[u'攻击范围']=data[u'攻击范围'].map({'远程':1,'近战':0})
# 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1
ss = StandardScaler()
data = ss.fit_transform(data)
# 构造GMM聚类
gmm = GaussianMixture(n_components=30, covariance_type='full')
gmm.fit(data)
# 训练数据
prediction = gmm.predict(data)
print(prediction)
# 将分组结果输出到CSV文件中
data_ori.insert(0, '分组', prediction)
data_ori.to_csv('./hero_out.csv', index=False, sep=',')
```

聚类和分类不一样，聚类是无监督的学习方式，也就是我们没有实际的结果可以进行比对，所以聚类的结果评估不像分类准确率一样直观，那么有没有聚类结果的评估方式呢？这里我们可以采用 Calinski-Harabaz 指标，代码如下：

```python

from sklearn.metrics import calinski_harabaz_score
print(calinski_harabaz_score(data, prediction))
#分数越高越好
```

![img](https://static001.geekbang.org/resource/image/43/d7/43b35b8f49ac83799ea1ca88383609d7.png)

## 10.关联规则挖掘：Apriori算法

关联规则这个概念，最早是由 Agrawal 等人在 1993 年提出的。在 1994 年 Agrawal 等人又提出了基于关联规则的 Apriori 算法，至今 Apriori 仍是关联规则挖掘的重要算法。

搞懂关联规则中的几个重要概念：**1、支持度、置信度、提升度；2、Apriori 算法的工作原理；3、在实际工作中，我们该如何进行关联规则挖掘**。

**例子：**超市购物商品购买列表

![img](https://static001.geekbang.org/resource/image/f7/1c/f7d0cc3c1a845bf790b344f62372941c.png)

支持度是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的频率越大。在这个例子中，我们能看到“牛奶”出现了 4 次，那么这 5 笔订单中“牛奶”的支持度就是 4/5=0.8。

置信度是当你购买了商品 A，会有多大的概率购买商品 B，在上面这个例子中，置信度（牛奶→啤酒）=2/4=0.5。

提升度是“商品 A 的出现，对商品 B 的出现概率提升的”程度，提升度 (A→B)= 置信度 (A→B)/ 支持度 (B)。所以提升度有三种可能：1、提升度 (A→B)>1：代表有提升；2、提升度 (A→B)=1：代表有没有提升，也没有下降；3、提升度 (A→B)<1：代表有下降。

把上面案例中的商品用 ID 来代表，牛奶、面包、尿布、可乐、啤酒、鸡蛋的商品 ID 分别设置为 1-6，上面的数据表可以变为：

![img](https://static001.geekbang.org/resource/image/e3/33/e30fe11a21191259e6a93568461fa933.png)

Apriori 算法其实就是查找频繁项集 (frequent itemset) 的过程，而频繁项集就是支持度大于等于最小支持度 (Min Support) 阈值的项集，所以小于最小值支持度的项目就是非频繁项集，而大于等于最小支持度的项集就是频繁项集。项集这个概念，英文叫做 itemset，它可以是单个的商品，也可以是商品的组合。

首先，我们先计算单个商品的支持度，也就是得到 K=1 项的支持度：

![img](https://static001.geekbang.org/resource/image/ff/de/fff5ba49aff930bba71c98685be4fcde.png)

因为最小支持度是 0.5，所以你能看到商品 4、6 是不符合最小支持度的，不属于频繁项集，于是经过筛选商品的频繁项集就变成：

![img](https://static001.geekbang.org/resource/image/ae/b6/ae108dc65c33e9ed9546a0d91bd881b6.png)

在这个基础上，我们将商品两两组合，得到 k=2 项的支持度

![img](https://static001.geekbang.org/resource/image/a5/a3/a51fd814ebd68304e3cb137630af3ea3.png)

我们再筛掉小于最小值支持度的商品组合，可以得到：

![img](https://static001.geekbang.org/resource/image/a0/c8/a087cd1bd2a9e033105de275834b79c8.png)

我们再将商品进行 K=3 项的商品组合，可以得到：

![img](https://static001.geekbang.org/resource/image/a7/9c/a7f4448cc5031b1edf304c9aed94039c.png)

再筛掉小于最小值支持度的商品组合，可以得到：

![img](https://static001.geekbang.org/resource/image/d5/0f/d51fc9137a537d8cb96fa21707cab70f.png)

通过上面这个过程，我们可以得到 K=3 项的频繁项集{1,2,3}，也就是{牛奶、面包、尿布}的组合。

Apriori 算法的递归流程：

1、K=1，计算 K 项集的支持度；2、筛选掉小于最小支持度的项集；3、如果项集为空，则对应 K-1 项集的结果为最终结果。否则 K=K+1，重复 1-3 步。

### **Apriori 的改进算法：FP-Growth 算法**

Apriori 在计算的过程中有以下几个缺点：

1、可能产生大量的候选集。因为采用排列组合的方式，把可能的项集都组合出来了；

2、每次计算都需要重新扫描数据集，来计算每个项集的支持度。

因此，产生了FP-Growth 算法，它的特点是：1、创建了一棵 FP 树来存储频繁项集。在创建前对不满足最小支持度的项进行删除，减少了存储空间。2、整个生成过程只遍历数据集 2 次，大大减少了计算量。

1. **创建项头表（item header table）**

创建项头表的作用是为 FP 构建及频繁项集挖掘提供索引。

这一步的流程是先扫描一遍数据集，对于满足最小支持度的单个项（K=1 项集）按照支持度从高到低进行排序，这个过程中删除了不满足最小支持度的项。

项头表包括了项目、支持度，以及该项在 FP 树中的链表。初始的时候链表为空。

![img](https://static001.geekbang.org/resource/image/69/f5/69ce07c61a654faafb4f5114df1557f5.png)

**2. 构造 FP 树**

FP 树的根节点记为 NULL 节点。整个流程是需要再次扫描数据集，对于每一条数据，按照支持度从高到低的顺序进行创建节点（也就是第一步中项头表中的排序结果），节点如果存在就将计数 count+1，如果不存在就进行创建。同时在创建的过程中，需要更新项头表的链表。

![img](https://static001.geekbang.org/resource/image/ea/92/eadaaf6585379815e62aad99386c7992.png)

**3. 通过 FP 树挖掘频繁项集**

到这里，我们就得到了一个存储频繁项集的 FP 树，以及一个项头表。我们可以通过项头表来挖掘出每个频繁项集。具体的操作会用到一个概念，叫“条件模式基”，它指的是以要挖掘的节点为叶子节点，自底向上求出 FP 子树，然后将 FP 子树的祖先节点设置为叶子节点之和。

我以“啤酒”的节点为例，从 FP 树中可以得到一棵 FP 子树，将祖先节点的支持度记为叶子节点之和，得到：

![img](https://static001.geekbang.org/resource/image/99/0f/9951cda824fc9823136231e7c8e70d0f.png)

你能看出来，相比于原来的 FP 树，尿布和牛奶的频繁项集数减少了。这是因为我们求得的是以“啤酒”为节点的 FP 子树，也就是说，在频繁项集中一定要含有“啤酒”这个项。你可以再看下原始的数据，其中订单 1{牛奶、面包、尿布}和订单 5{牛奶、面包、尿布、可乐}并不存在“啤酒”这个项，所以针对订单 1，尿布→牛奶→面包这个项集就会从 FP 树中去掉，针对订单 5 也包括了尿布→牛奶→面包这个项集也会从 FP 树中去掉，所以你能看到以“啤酒”为节点的 FP 子树，尿布、牛奶、面包项集上的计数比原来少了 2。条件模式基不包括“啤酒”节点，而且祖先节点如果小于最小支持度就会被剪枝，所以“啤酒”的条件模式基为空。

同理，我们可以求得“面包”的条件模式基为：

![img](https://static001.geekbang.org/resource/image/41/13/41026c8f25b64b01125c8b8d6a19a113.png)

所以可以求得面包的频繁项集为{尿布，面包}，{尿布，牛奶，面包}。

![img](https://static001.geekbang.org/resource/image/c7/35/c7aee3b17269139ed3d5a6b82cc56735.png)



### **Apriori算法的实践**

```python
pip install efficient-apriori

itemsets, rules = apriori(data, min_support,  min_confidence)
#其中 data 是我们要提供的数据集，它是一个 list 数组类型。min_support 参数为最小支持度，在 efficient-apriori 工具包中用 0 到 1 的数值代表百分比，比如 0.5 代表最小支持度为 50%。min_confidence 是最小置信度，数值也代表百分比，比如 1 代表 100%。

from efficient_apriori import apriori
# 设置数据集
data = [('牛奶','面包','尿布'),
           ('可乐','面包', '尿布', '啤酒'),
           ('牛奶','尿布', '啤酒', '鸡蛋'),
           ('面包', '牛奶', '尿布', '啤酒'),
           ('面包', '牛奶', '尿布', '可乐')]
# 挖掘频繁项集和频繁规则
itemsets, rules = apriori(data, min_support=0.5,  min_confidence=1)
print(itemsets)
print(rules)

```

```python
#从豆瓣电影抓取某个导演的电影及演员

# -*- coding: utf-8 -*-
# 下载某个导演的电影数据集
from efficient_apriori import apriori
from lxml import etree
import time
from selenium import webdriver
import csv
driver = webdriver.Chrome()
# 设置想要下载的导演 数据集
director = u'宁浩'
# 写CSV文件
file_name = './' + director + '.csv'
base_url = 'https://movie.douban.com/subject_search?search_text='+director+'&cat=1002&start='
out = open(file_name,'w', newline='', encoding='utf-8-sig')
csv_write = csv.writer(out, dialect='excel')
flags=[]
# 下载指定页面的数据
def download(request_url):
  driver.get(request_url)
  time.sleep(1)
  html = driver.find_element_by_xpath("//*").get_attribute("outerHTML")
  html = etree.HTML(html)
  # 设置电影名称，导演演员 的XPATH
  movie_lists = html.xpath("/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='title']/a[@class='title-text']")
  name_lists = html.xpath("/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='meta abstract_2']")
  # 获取返回的数据个数
  num = len(movie_lists)
  if num > 15: #第一页会有16条数据
    # 默认第一个不是，所以需要去掉
    movie_lists = movie_lists[1:]
    name_lists = name_lists[1:]
  for (movie, name_list) in zip(movie_lists, name_lists):
    # 会存在数据为空的情况
    if name_list.text is None: 
      continue
    # 显示下演员名称
    print(name_list.text)
    names = name_list.text.split('/')
    # 判断导演是否为指定的director
    if names[0].strip() == director and movie.text not in flags:
      # 将第一个字段设置为电影名称
      names[0] = movie.text
      flags.append(movie.text)
      csv_write.writerow(names)
  print('OK') # 代表这页数据下载成功
  print(num)
  if num >= 14: #有可能一页会有14个电影
    # 继续下一页
    return True
  else:
    # 没有下一页
    return False

# 开始的ID为0，每页增加15
start = 0
while start<10000: #最多抽取1万部电影
  request_url = base_url + str(start)
  # 下载数据，并返回是否有下一页
  flag = download(request_url)
  if flag:
    start = start + 15
  else:
    break
out.close()
print('finished')
```

```python
#用 Apriori 算法来挖掘频繁项集和关联规则，代码如下：

# -*- coding: utf-8 -*-
from efficient_apriori import apriori
import csv
director = u'宁浩'
file_name = './'+director+'.csv'
lists = csv.reader(open(file_name, 'r', encoding='utf-8-sig'))
# 数据加载
data = []
for names in lists:
     name_new = []
     for name in names:
           # 去掉演员数据中的空格
           name_new.append(name.strip())
     data.append(name_new[1:])
# 挖掘频繁项集和关联规则
itemsets, rules = apriori(data, min_support=0.5,  min_confidence=1)
print(itemsets)
print(rules)
```

![img](https://static001.geekbang.org/resource/image/28/9d/282c25e8651b3e0b675be7267d13629d.png)



## PageRank算法

PageRank 算法由Google 的创始人拉里·佩奇提出，目的就是要找到优质的网页，这样 Google 的排序结果不仅能找到用户想要的内容，而且还会从众多网页中筛选出权重高的呈现给用户。该算法受到了论文影响力因子的评价启发。当一篇论文被引用的次数越多，证明这篇论文的影响力越大。

### **PageRank 的简化模型**

我假设一共有 4 个网页 A、B、C、D。它们之间的链接信息如图所示：

![img](https://static001.geekbang.org/resource/image/81/36/814d53ff8d73113631482e71b7c53636.png)

出链指的是链接出去的链接。入链指的是链接进来的链接。比如图中 A 有 2 个入链，3 个出链。

简单来说，一个网页的影响力 = 所有入链集合的页面的加权影响力之和，用公式表示为：
$$
PR(U) = \sum_{v\in B_u} \frac{PR(v)}{L(v)}
$$
u 为待评估的页面，Bu 为页面 u 的入链集合。针对入链集合中的任意页面 v，它能给 u 带来的影响力是其自身的影响力 PR(v) 除以 v 页面的出链数量，即页面 v 把影响力 PR(v) 平均分配给了它的出链，这样统计所有能给 u 带来链接的页面 v，得到的总和就是网页 u 的影响力，即为 PR(u)。

在这个例子中，你能看到 A 有三个出链分别链接到了 B、C、D 上。那么当用户访问 A 的时候，就有跳转到 B、C 或者 D 的可能性，跳转概率均为 1/3。

B 有两个出链，链接到了 A 和 D 上，跳转概率为 1/2。

这样，我们可以得到 A、B、C、D 这四个网页的转移矩阵 M：
$$
\left[ \begin{matrix} 0 & 1/2 & 1 & 0\\ 1/3 & 0 & 0 & 1/2\\ 1/3 & 0 & 0 & 1/2\\ 1/3 & 1/2 & 0 & 0\end{matrix} \right]
$$
我们假设 A、B、C、D 四个页面的初始影响力都是相同的，即：
$$
w_0 = \left[ \begin{matrix} 1/4\\ 1/4\\1/4\\ 1/4\end{matrix} \right]
$$
当进行第一次转移之后，各页面的影响力 w1 变为：
$$
w_1 =  Mw_0 =\left[ \begin{matrix} 0 & 1/2 & 1 & 0\\ 1/3 & 0 & 0 & 1/2\\ 1/3 & 0 & 0 & 1/2\\ 1/3 & 1/2 & 0 & 0\end{matrix} \right] \left[ \begin{matrix} 1/4\\ 1/4\\1/4\\ 1/4\end{matrix} \right] = \left[ \begin{matrix} 9/24\\ 5/24\\5/24\\ 5/24\end{matrix} \right]
$$
然后我们再用转移矩阵乘以 w1 得到 w2 结果，直到第 n 次迭代后 wn 影响力不再发生变化，可以收敛到 (0.3333，0.2222，0.2222，0.2222），也就是对应着 A、B、C、D 四个页面最终平衡状态下的影响力。你能看出 A 页面相比于其他页面来说权重更大，也就是 PR 值更高。而 B、C、D 页面的 PR 值相等。

至此，我们模拟了一个简化的 PageRank 的计算过程，实际情况会比这个复杂，可能会面临两个问题：

1. 等级泄露（Rank Leak）：如果一个网页没有出链，就像是一个黑洞一样，吸收了其他网页的影响力而不释放，最终会导致其他网页的 PR 值为 0。

![img](https://static001.geekbang.org/resource/image/77/62/77336108b0233638a35bfd7450438162.png)

2. 等级沉没（Rank Sink）：如果一个网页只有出链，没有入链（如下图所示），计算的过程迭代下来，会导致这个网页的 PR 值为 0（也就是不存在公式中的 V）。

![img](https://static001.geekbang.org/resource/image/0d/e6/0d113854fb56116d79efe7f0e0374fe6.png)

比如针对等级泄露的情况，我们可以把没有出链的节点，先从图中去掉，等计算完所有节点的 PR 值之后，再加上该节点进行计算。不过这种方法会导致新的等级泄露的节点的产生，所以工作量还是很大的。

### **PageRank 的随机浏览模型**

为了解决简化模型中存在的等级泄露和等级沉没的问题，拉里·佩奇提出了 PageRank 的随机浏览模型。他假设了这样一个场景：用户并不都是按照跳转链接的方式来上网，还有一种可能是不论当前处于哪个页面，都有概率访问到其他任意的页面，比如说用户就是要直接输入网址访问其他页面，虽然这个概率比较小。

所以他定义了阻尼因子 d，这个因子代表了用户按照跳转链接来上网的概率，通常可以取一个固定值 0.85，而 1-d=0.15 则代表了用户不是通过跳转链接的方式来访问网页的，比如直接输入网址。
$$
PR(u) = \frac{1-d}{N}+d\sum_{v \in B_u} \frac{PR(v)}{L(v)}
$$
其中 N 为网页总数，这样我们又可以重新迭代网页的权重计算了，因为加入了阻尼因子 d，一定程度上解决了等级泄露和等级沉没的问题。通过数学定理（这里不进行讲解）也可以证明，最终 PageRank 随机浏览模型是可以收敛的，也就是可以得到一个稳定正常的 PR 值。

![img](https://static001.geekbang.org/resource/image/f9/7d/f936296fed70f27ba23064ec14a7e37d.png)



### **PageRank算法实战**

在这之前，你需要思考三个问题：

1、如何使用工具完成 PageRank 算法，包括使用工具创建网络图，设置节点、边、权重等，并通过创建好的网络图计算节点的 PR 值；

2、对于一个实际的项目，比如希拉里的 9306 封邮件（工具包中邮件的数量），如何使用 PageRank 算法挖掘出有影响力的节点，并且绘制网络图；

3、如何对创建好的网络图进行可视化，如果网络中的节点数较多，如何筛选重要的节点进行可视化，从而得到精简的网络关系图。

**如何使用工具实现 PageRank 算法**

PageRank 算法工具在 sklearn 中并不存在，我们需要找到新的工具包。实际上有一个关于图论和网络建模的工具叫 NetworkX，它是用 Python 语言开发的工具，内置了常用的图与网络分析算法，可以方便我们进行网络数据分析。

![img](https://static001.geekbang.org/resource/image/47/ea/47e5f21d16b15a98d4a32a73ebd477ea.png)

```python

import networkx as nx
# 创建有向图
G = nx.DiGraph() 
# 有向图之间边的关系
edges = [("A", "B"), ("A", "C"), ("A", "D"), ("B", "A"), ("B", "D"), ("C", "A"), ("D", "B"), ("D", "C")]
for edge in edges:
    G.add_edge(edge[0], edge[1])
pagerank_list = nx.pagerank(G, alpha=1)
print("pagerank值是：", pagerank_list)
```

**NetworkX 工具常用的操作：**

1. 关于图的创建图可以分为无向图和有向图，在 NetworkX 中分别采用不同的函数进行创建。无向图指的是不用节点之间的边的方向，使用 nx.Graph() 进行创建；有向图指的是节点之间的边是有方向的，使用 nx.DiGraph() 来创建。在上面这个例子中，存在 A→D 的边，但不存在 D→A 的边。

2. 关于节点的增加、删除和查询如果想在网络中增加节点，可以使用 G.add_node(‘A’) 添加一个节点，也可以使用 G.add_nodes_from([‘B’,‘C’,‘D’,‘E’]) 添加节点集合。如果想要删除节点，可以使用 G.remove_node(node) 删除一个指定的节点，也可以使用 G.remove_nodes_from([‘B’,‘C’,‘D’,‘E’]) 删除集合中的节点。那么该如何查询节点呢？如果你想要得到图中所有的节点，就可以使用 G.nodes()，也可以用 G.number_of_nodes() 得到图中节点的个数。

3. 关于边的增加、删除、查询增加边与添加节点的方式相同，使用 G.add_edge(“A”, “B”) 添加指定的“从 A 到 B”的边，也可以使用 add_edges_from 函数从边集合中添加。我们也可以做一个加权图，也就是说边是带有权重的，使用 add_weighted_edges_from 函数从带有权重的边的集合中添加。在这个函数的参数中接收的是 1 个或多个三元组[u,v,w]作为参数，u、v、w 分别代表起点、终点和权重。另外，我们可以使用 remove_edge 函数和 remove_edges_from 函数删除指定边和从边集合中删除。另外可以使用 edges() 函数访问图中所有的边，使用 number_of_edges() 函数得到图中边的个数。

   以上是关于图的基本操作，如果我们创建了一个图，并且对节点和边进行了设置，就可以找到其中有影响力的节点，原理就是通过 PageRank 算法，使用 nx.pagerank(G) 这个函数，函数中的参数 G 代表创建好的图。

#### **用 PageRank 揭秘希拉里邮件中的人物关系**

希拉里邮件事件数据集：https://github.com/cystanford/PageRank。

整个数据集由三个文件组成：Aliases.csv，Emails.csv 和 Persons.csv，其中 Emails 文件记录了所有公开邮件的内容，发送者和接收者的信息。Persons 这个文件统计了邮件中所有人物的姓名及对应的 ID。因为姓名存在别名的情况，为了将邮件中的人物进行统一，我们还需要用 Aliases 文件来查询别名和人物的对应关系。整个数据集包括了 9306 封邮件和 513 个人名，数据集还是比较大的。不过这一次我们不需要对邮件的内容进行分析，只需要通过邮件中的发送者和接收者（对应 Emails.csv 文件中的 MetadataFrom 和 MetadataTo 字段）来绘制整个关系网络。因为涉及到的人物很多，因此我们需要通过 PageRank 算法计算每个人物在邮件关系网络中的权重，最后筛选出来最有价值的人物来进行关系网络图的绘制

![img](https://static001.geekbang.org/resource/image/72/c9/72132ffbc1209301f0876178c75927c9.jpg)



1、首先我们需要加载数据源；

2、在准备阶段：我们需要对数据进行探索，在数据清洗过程中，因为邮件中存在别名的情况，因此我们需要统一人物名称。另外邮件的正文并不在我们考虑的范围内，只统计邮件中的发送者和接收者，因此我们筛选 MetadataFrom 和 MetadataTo 这两个字段作为特征。同时，发送者和接收者可能存在多次邮件往来，需要设置权重来统计两人邮件往来的次数。次数越多代表这个边（从发送者到接收者的边）的权重越高；

3、在挖掘阶段：我们主要是对已经设置好的网络图进行 PR 值的计算，但邮件中的人物有 500 多人，有些人的权重可能不高，我们需要筛选 PR 值高的人物，绘制出他们之间的往来关系。在可视化的过程中，我们可以通过节点的 PR 值来绘制节点的大小，PR 值越大，节点的绘制尺寸越大。设置好流程之后，实现的代码如下：

```python

# -*- coding: utf-8 -*-
# 用 PageRank 挖掘希拉里邮件中的重要任务关系
import pandas as pd
import networkx as nx
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt
# 数据加载
emails = pd.read_csv("./input/Emails.csv")
# 读取别名文件
file = pd.read_csv("./input/Aliases.csv")
aliases = {}
for index, row in file.iterrows():
    aliases[row['Alias']] = row['PersonId']
# 读取人名文件
file = pd.read_csv("./input/Persons.csv")
persons = {}
for index, row in file.iterrows():
    persons[row['Id']] = row['Name']
# 针对别名进行转换        
def unify_name(name):
    # 姓名统一小写
    name = str(name).lower()
    # 去掉, 和 @后面的内容
    name = name.replace(",","").split("@")[0]
    # 别名转换
    if name in aliases.keys():
        return persons[aliases[name]]
    return name
# 画网络图
def show_graph(graph, layout='spring_layout'):
    # 使用 Spring Layout 布局，类似中心放射状
    if layout == 'circular_layout':
        positions=nx.circular_layout(graph)
    else:
        positions=nx.spring_layout(graph)
    # 设置网络图中的节点大小，大小与 pagerank 值相关，因为 pagerank 值很小所以需要 *20000
    nodesize = [x['pagerank']*20000 for v,x in graph.nodes(data=True)]
    # 设置网络图中的边长度
    edgesize = [np.sqrt(e[2]['weight']) for e in graph.edges(data=True)]
    # 绘制节点
    nx.draw_networkx_nodes(graph, positions, node_size=nodesize, alpha=0.4)
    # 绘制边
    nx.draw_networkx_edges(graph, positions, edge_size=edgesize, alpha=0.2)
    # 绘制节点的 label
    nx.draw_networkx_labels(graph, positions, font_size=10)
    # 输出希拉里邮件中的所有人物关系图
    plt.show()
# 将寄件人和收件人的姓名进行规范化
emails.MetadataFrom = emails.MetadataFrom.apply(unify_name)
emails.MetadataTo = emails.MetadataTo.apply(unify_name)
# 设置遍的权重等于发邮件的次数
edges_weights_temp = defaultdict(list)
for row in zip(emails.MetadataFrom, emails.MetadataTo, emails.RawText):
    temp = (row[0], row[1])
    if temp not in edges_weights_temp:
        edges_weights_temp[temp] = 1
    else:
        edges_weights_temp[temp] = edges_weights_temp[temp] + 1
# 转化格式 (from, to), weight => from, to, weight
edges_weights = [(key[0], key[1], val) for key, val in edges_weights_temp.items()]
# 创建一个有向图
graph = nx.DiGraph()
# 设置有向图中的路径及权重 (from, to, weight)
graph.add_weighted_edges_from(edges_weights)
# 计算每个节点（人）的 PR 值，并作为节点的 pagerank 属性
pagerank = nx.pagerank(graph)
# 将 pagerank 数值作为节点的属性
nx.set_node_attributes(graph, name = 'pagerank', values=pagerank)
# 画网络图
show_graph(graph)

# 将完整的图谱进行精简
# 设置 PR 值的阈值，筛选大于阈值的重要核心节点
pagerank_threshold = 0.005
# 复制一份计算好的网络图
small_graph = graph.copy()
# 剪掉 PR 值小于 pagerank_threshold 的节点
for n, p_rank in graph.nodes(data=True):
    if p_rank['pagerank'] < pagerank_threshold: 
        small_graph.remove_node(n)
# 画网络图,采用circular_layout布局让筛选出来的点组成一个圆
show_graph(small_graph, 'circular_layout')
```

结果如下：

![img](https://static001.geekbang.org/resource/image/41/b1/419f7621392045f07bcd03f9e4c7c8b1.png)

![img](https://static001.geekbang.org/resource/image/3f/e1/3f08f61360e8a82a23a16e44d2b973e1.png)

1. **函数定义人物的名称需要统一**，因此我设置了 unify_name 函数，同时设置了 show_graph 函数将网络图可视化。NetworkX 提供了多种可视化布局，这里我使用 spring_layout 布局，也就是呈中心放射状。除了 spring_layout 外，NetworkX 还有另外三种可视化布局，circular_layout（在一个圆环上均匀分布节点），random_layout（随机分布节点 ），shell_layout（节点都在同心圆上）。
2. **计算边权重邮件的发送者和接收者的邮件往来可能不止一次**，我们需要用两者之间邮件往来的次数计算这两者之间边的权重，所以我用 edges_weights_temp 数组存储权重。而上面介绍过在 NetworkX 中添加权重边（即使用 add_weighted_edges_from 函数）的时候，接受的是 u、v、w 的三元数组，因此我们还需要对格式进行转换，具体转换方式见代码。
3. PR 值计算及筛选我使用 nx.pagerank(graph) 计算了节点的 PR 值。由于节点数量很多，我们设置了 PR 值阈值，即 pagerank_threshold=0.005，然后遍历节点，删除小于 PR 值阈值的节点，形成新的图 small_graph，最后对 small_graph 进行可视化（对应运行结果的第二张图）。

![img](https://static001.geekbang.org/resource/image/30/42/307055050e005ba5092028a074a5c142.png)



## AdaBoost算法

在数据挖掘中，分类算法可以说是核心算法，其中 AdaBoost 算法与随机森林算法一样都属于分类算法中的集成算法。

集成的含义就是集思广益，博取众长，当我们做决定的时候，我们先听取多个专家的意见，再做决定。集成算法通常有两种方式，分别是投票选举（bagging）和再学习（boosting）。投票选举的场景类似把专家召集到一个会议桌前，当做一个决定的时候，让 K 个专家（K 个模型）分别进行分类，然后选择出现次数最多的那个类作为最终的分类结果。再学习相当于把 K 个专家（K 个分类器）进行加权融合，形成一个新的超级专家（强分类器），让这个超级专家做判断。

Boosting 的含义是提升，它的作用是每一次训练的时候都对上一次的训练进行改进提升，在训练的过程中这 K 个“专家”之间是有依赖性的，当引入第 K 个“专家”（第 K 个分类器）的时候，实际上是对前 K-1 个专家的优化。而 bagging 在做投票选举的时候可以并行计算，也就是 K 个“专家”在做判断的时候是相互独立的，不存在依赖性。

### **AdaBoost 的工作原理**

AdaBoost 的英文全称是 Adaptive Boosting，中文含义是自适应提升算法。它由 Freund 等人于 1995 年提出，是对 Boosting 算法的一种实现。这类算法通过训练多个弱分类器，将它们组合成一个强分类器。

假设弱分类器为 Gi(x)，它在强分类器中的权重 αi，那么就可以得出强分类器 f(x)：
$$
f(x)=\sum_{i=1}^{n} \alpha_i G_i(x)
$$
Question:1、如何得到弱分类器，也就是在每次迭代训练的过程中，如何得到最优弱分类器？2、每个弱分类器在强分类器中的权重是如何计算的？

我们先来看下第二个问题。实际上在一个由 K 个弱分类器中组成的强分类器中，如果弱分类器的分类效果好，那么权重应该比较大，如果弱分类器的分类效果一般，权重应该降低。所以我们需要基于这个弱分类器对样本的分类错误率来决定它的权重，用公式表示就是：
$$
\alpha_i = \frac{1}{2}log\frac{1-e_i}{e_i}
$$
其中 ei 代表第 i 个分类器的分类错误率。

然后我们再来看下第一个问题，如何在每次训练迭代的过程中选择最优的弱分类器？

实际上，AdaBoost 算法是通过改变样本的数据分布来实现的。AdaBoost 会判断每次训练的样本是否正确分类，对于正确分类的样本，降低它的权重，对于被错误分类的样本，增加它的权重。再基于上一次得到的分类准确率，来确定这次训练样本中每个样本的权重。然后将修改过权重的新数据集传递给下一层的分类器进行训练。这样做的好处就是，通过每一轮训练样本的动态权重，可以让训练的焦点集中到难分类的样本上，最终得到的弱分类器的组合更容易得到更高的分类准确率。

我们可以用 Dk+1 代表第 k+1 轮训练中，样本的权重集合，其中 Wk+1,1 代表第 k+1 轮中第一个样本的权重，以此类推 Wk+1,N 代表第 k+1 轮中第 N 个样本的权重，因此用公式表示为：
$$
D_{k+1} = (w_{k+1,1},w_{k+1,2},...,w_{k+1,N})
$$
第 k+1 轮中的样本权重，是根据该样本在第 k 轮的权重以及第 k 个分类器的准确率而定，具体的公式为：
$$
w_{k+1,i} = \frac{w_k.i}{Z_k}exp(-\alpha_k y_i G_k(x_i),i=1,2,...,N)
$$
其中$\Z_k$代表规范化因子，为了让样本权重和为1，需要初一规范化银子，所以：
$$
Z_k =\sum_{i=1}^{N} w_{k,i}exp(-\alpha_ky_iG_k(x_i))
$$

### **AdaBoost 算法示例**

了解 AdaBoost 的工作原理之后，我们看一个例子，假设我有 10 个训练样本，如下所示：

![img](https://static001.geekbang.org/resource/image/73/38/734c8272df1f96903be1777733a10f38.png)

现在我希望通过 AdaBoost 构建一个强分类器。按照上面的 AdaBoost 工作原理，我们来模拟一下。首先在第一轮训练中，我们得到 10 个样本的权重为 1/10，即初始的 10 个样本权重一致，D1=(0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1)。

假设我有 3 个基础分类器：

![img](https://static001.geekbang.org/resource/image/32/a4/325756eb08b5b3fd55402c9a8ba4dca4.png)

我们可以知道分类器 f1 的错误率为 0.3，也就是 x 取值 6、7、8 时分类错误；分类器 f2 的错误率为 0.4，即 x 取值 0、1、2、9 时分类错误；分类器 f3 的错误率为 0.3，即 x 取值为 3、4、5 时分类错误。

这 3 个分类器中，f1、f3 分类器的错误率最低，因此我们选择 f1 或 f3 作为最优分类器，假设我们选 f1 分类器作为最优分类器，即第一轮训练得到：

![img](https://static001.geekbang.org/resource/image/3d/fb/3dd329577aef1a810a1c130095a3e0fb.png)

根据分类器权重公式得到：

![img](https://static001.geekbang.org/resource/image/f9/60/f92e515d7ad7c1ee5f3bf45574bf3060.png)

然后我们对下一轮的样本更新求权重值，代入 Wk+1,i 和 Dk+1 的公式，可以得到新的权重矩阵：D2=(0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.1666, 0.1666, 0.1666, 0.0715)。

在第二轮训练中，我们继续统计三个分类器的准确率，可以得到分类器 f1 的错误率为 0.1666*3，也就是 x 取值为 6、7、8 时分类错误。分类器 f2 的错误率为 0.0715*4，即 x 取值为 0、1、2、9 时分类错误。分类器 f3 的错误率为 0.0715*3，即 x 取值 3、4、5 时分类错误。

在这 3 个分类器中，f3 分类器的错误率最低，因此我们选择 f3 作为第二轮训练的最优分类器，即：

![img](https://static001.geekbang.org/resource/image/68/40/687202173085a62e2c7b32deb05e9440.png)

根据分类器权重公式得到：

![img](https://static001.geekbang.org/resource/image/ce/8b/ce8a4e319726f159104681a4152e3a8b.png)

同样，我们对下一轮的样本更新求权重值，代入 Wk+1,i 和 Dk+1 的公式，可以得到 D3=(0.0455,0.0455,0.0455,0.1667, 0.1667,0.01667,0.1060, 0.1060, 0.1060, 0.0455)。

在第三轮训练中，我们继续统计三个分类器的准确率，可以得到分类器 f1 的错误率为 0.1060*3，也就是 x 取值 6、7、8 时分类错误。分类器 f2 的错误率为 0.0455*4，即 x 取值为 0、1、2、9 时分类错误。分类器 f3 的错误率为 0.1667*3，即 x 取值 3、4、5 时分类错误。

在这 3 个分类器中，f2 分类器的错误率最低，因此我们选择 f2 作为第三轮训练的最优分类器，即：

![img](https://static001.geekbang.org/resource/image/88/15/8847a9e60b38a79c08086e1620d6d915.png)

我们根据分类器权重公式得到：

![img](https://static001.geekbang.org/resource/image/0e/c3/0efb64e73269ee142cde91de532627c3.png)

假设我们只进行 3 轮的训练，选择 3 个弱分类器，组合成一个强分类器，那么最终的强分类器 G(x) = 0.4236G1(x) + 0.6496G2(x)+0.7514G3(x)。

实际上 AdaBoost 算法是一个框架，你可以指定任意的分类器，通常我们可以采用 CART 分类器作为弱分类器。通过上面这个示例的运算，你体会一下 AdaBoost 的计算流程即可。

![img](https://static001.geekbang.org/resource/image/10/00/10ddea37b3fdea2ec019f38b59ac6b00.png)



### **AdaBoost 算法实战**

AdaBoost 不仅可以用于分类问题，还可以用于回归分析。

在 sklearn 中创建 AdaBoost 分类器。我们需要使用 AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm=’SAMME.R’, random_state=None) 这个函数，

其中有几个比较主要的参数，我分别来讲解下：

1.base_estimator：代表的是弱分类器。在 AdaBoost 的分类器和回归器中都有这个参数，在 AdaBoost 中默认使用的是决策树，一般我们不需要修改这个参数，当然你也可以指定具体的分类器。

2.n_estimators：算法的最大迭代次数，也是分类器的个数，每一次迭代都会引入一个新的弱分类器来增加原有的分类器的组合能力。默认是 50。

3.learning_rate：代表学习率，取值在 0-1 之间，默认是 1.0。如果学习率较小，就需要比较多的迭代次数才能收敛，也就是说学习率和迭代次数是有相关性的。当你调整 learning_rate 的时候，往往也需要调整 n_estimators 这个参数。

4.algorithm：代表我们要采用哪种 boosting 算法，一共有两种选择：SAMME 和 SAMME.R。默认是 SAMME.R。这两者之间的区别在于对弱分类权重的计算方式不同。

5.random_state：代表随机数种子的设置，默认是 None。随机种子是用来控制随机模式的，当随机种子取了一个值，也就确定了一种随机规则，其他人取这个值可以得到同样的结果。如果不设置随机种子，每次得到的随机数也就不同。

那么如何创建 AdaBoost 回归呢？我们可以使用 AdaBoostRegressor(base_estimator=None, n_estimators=50, learning_rate=1.0, loss=‘linear’, random_state=None) 这个函数。你能看出来回归和分类的参数基本是一致的，不同点在于回归算法里没有 algorithm 这个参数，但多了一个 loss 参数。loss 代表损失函数的设置，一共有 3 种选择，分别为 linear、square 和 exponential，它们的含义分别是线性、平方和指数。默认是线性。一般采用线性就可以得到不错的效果。

创建好 AdaBoost 分类器或回归器之后，我们就可以输入训练集对它进行训练。我们使用 fit 函数，传入训练集中的样本特征值 train_X 和结果 train_y，模型会自动拟合。使用 predict 函数进行预测，传入测试集中的样本特征值 test_X，然后就可以得到预测结果。

#### **用 AdaBoost 对房价进行预测**

数据集用的是sklearn自带的波士顿房价数据

![img](https://static001.geekbang.org/resource/image/42/b7/426dec532f34d7f458e36ee59a6617b7.png)



```python

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_boston
from sklearn.ensemble import AdaBoostRegressor
# 加载数据
data=load_boston()
# 分割数据
train_x, test_x, train_y, test_y = train_test_split(data.data, data.target, test_size=0.25, random_state=33)
# 使用AdaBoost回归模型
regressor=AdaBoostRegressor()
regressor.fit(train_x,train_y)
pred_y = regressor.predict(test_x)
mse = mean_squared_error(test_y, pred_y)
print("房价预测结果 ", pred_y)
print("均方误差 = ",round(mse,2))
```

同样，我们可以使用不同的回归分析模型分析这个数据集，比如使用决策树回归和 KNN 回归。

```python

# 使用决策树回归模型
dec_regressor=DecisionTreeRegressor()
dec_regressor.fit(train_x,train_y)
pred_y = dec_regressor.predict(test_x)
mse = mean_squared_error(test_y, pred_y)
print("决策树均方误差 = ",round(mse,2))
# 使用KNN回归模型
knn_regressor=KNeighborsRegressor()
knn_regressor.fit(train_x,train_y)
pred_y = knn_regressor.predict(test_x)
mse = mean_squared_error(test_y, pred_y)
print("KNN均方误差 = ",round(mse,2))
```

相比之下，AdaBoost 的均方误差更小，也就是结果更优。虽然 AdaBoost 使用了弱分类器，但是通过 50 个甚至更多的弱分类器组合起来而形成的强分类器，在很多情况下结果都优于其他算法。因此 AdaBoost 也是常用的分类和回归算法之一。

### **AdaBoost 与决策树模型的比较**

在 sklearn 中 AdaBoost 默认采用的是决策树模型，我们可以随机生成一些数据，然后对比下 AdaBoost 中的弱分类器（也就是决策树弱分类器）、决策树分类器和 AdaBoost 模型在分类准确率上的表现。

如果想要随机生成数据，我们可以使用 sklearn 中的 make_hastie_10_2 函数生成二分类数据。假设我们生成 12000 个数据，取前 2000 个作为测试集，其余作为训练集。

有了数据和训练模型后，我们就可以编写代码。我设置了 AdaBoost 的迭代次数为 200，代表 AdaBoost 由 200 个弱分类器组成。针对训练集，我们用三种模型分别进行训练，然后用测试集进行预测，并将三个分类器的错误率进行可视化对比，可以看到这三者之间的区别：

```python

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.metrics import zero_one_loss
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import  AdaBoostClassifier
# 设置AdaBoost迭代次数
n_estimators=200
# 使用
X,y=datasets.make_hastie_10_2(n_samples=12000,random_state=1)
# 从12000个数据中取前2000行作为测试集，其余作为训练集
train_x, train_y = X[2000:],y[2000:]
test_x, test_y = X[:2000],y[:2000]
# 弱分类器
dt_stump = DecisionTreeClassifier(max_depth=1,min_samples_leaf=1)
dt_stump.fit(train_x, train_y)
dt_stump_err = 1.0-dt_stump.score(test_x, test_y)
# 决策树分类器
dt = DecisionTreeClassifier()
dt.fit(train_x,  train_y)
dt_err = 1.0-dt.score(test_x, test_y)
# AdaBoost分类器
ada = AdaBoostClassifier(base_estimator=dt_stump,n_estimators=n_estimators)
ada.fit(train_x,  train_y)
# 三个分类器的错误率可视化
fig = plt.figure()
# 设置plt正确显示中文
plt.rcParams['font.sans-serif'] = ['SimHei']
ax = fig.add_subplot(111)
ax.plot([1,n_estimators],[dt_stump_err]*2, 'k-', label=u'决策树弱分类器 错误率')
ax.plot([1,n_estimators],[dt_err]*2,'k--', label=u'决策树模型 错误率')
ada_err = np.zeros((n_estimators,))
# 遍历每次迭代的结果 i为迭代次数, pred_y为预测结果
for i,pred_y in enumerate(ada.staged_predict(test_x)):
     # 统计错误率
    ada_err[i]=zero_one_loss(pred_y, test_y)
# 绘制每次迭代的AdaBoost错误率 
ax.plot(np.arange(n_estimators)+1, ada_err, label='AdaBoost Test 错误率', color='orange')
ax.set_xlabel('迭代次数')
ax.set_ylabel('错误率')
leg=ax.legend(loc='upper right',fancybox=True)
plt.show()
```



![img](https://static001.geekbang.org/resource/image/8a/35/8ad4bb6a8c6848f2061ff6f442568735.png)

从图中你能看出来，弱分类器的错误率最高，只比随机分类结果略好，准确率稍微大于 50%。决策树模型的错误率明显要低很多。而 AdaBoost 模型在迭代次数超过 25 次之后，错误率有了明显下降，经过 125 次迭代之后错误率的变化形势趋于平缓。因此我们能看出，虽然单独的一个决策树弱分类器效果不好，但是多个决策树弱分类器组合起来形成的 AdaBoost 分类器，分类效果要好于决策树模型。

![img](https://static001.geekbang.org/resource/image/6c/17/6c4fcd75a65dc354bc65590c18e77d17.png)

## 7.神经网络

基本单位是神经元
$$
o(x_1,x_2...,x_n)=\left\{
\begin{aligned}
&1&,& if \quad w_0+w_1x_1+...+w_nx_n>0 \\
&0&,& otherwise
\end{aligned}
\right.
$$

$$
\begin{aligned}
误差：E(\overrightarrow{w}) &= \frac{1}{2}\sum \limits_{d\in D}(t_d-o_d)^2,其中t_d为期望输出，o_d为实际输出 \\
\triangle E(\overrightarrow{w}) &= [\frac{\partial E}{\partial w_0},\frac{\partial E}{\partial w_1},...\frac{\partial E}{\partial w_n}] (注：表示误差对某个方向的偏量) \\
w_i &= w_i + \triangle w_i  \quad  \\
where \quad \triangle w_i &= -\eta \frac{\partial E}{\partial w_i},\eta 表示学习速度(learning rate)
\end{aligned}
$$

$$
\begin{aligned}
\frac{\partial E}{\partial w_i} &= \frac{\partial}{\partial w_i} \frac{1}{2} \sum \limits_{d \in D}(t_d-o_d)^2\\
&=\frac{1}{2} \sum \limits_{d \in D} \frac{\partial}{\partial w_i} (t_d-o_d)^2 \\
&=\sum \limits_{d \in D} (t_d-o_d) \frac{\partial}{\partial w_i} (t_d-o_d),其中假设o_d(x) = w*x，线性函数 \\
&=-\sum \limits_{d \in D} (t_d-o_d)x_{id}
\end{aligned}
$$

所以，$\triangle w_i=\eta\sum \limits_{d \in D} (t_d-o_d)x_{id}$

**Batch Learning**:

​	1. Initialize each $w_i$ to some small random value

 2.  Until the termination condition is met, Do

     ​		1) Initialize each $\triangle w_i$ to zero

     ​		2) For each <x,t> in traing_example, Do

     ​				. Input the instance x to the unit and compute the output o

     ​				. For each linear unit weight $w_{it}$, Do

     ​						$-\triangle w_i  \longleftarrow \triangle w_i + \eta(t-o)x_i $

     ​		3) For each linear unit weight $w_{it}$, Do 

     ​						$w_i \longleftarrow w_i + \triangle w_i$

### **Backpropagation Rule(BP算法，反向传播算法)**：

​			即通过输出元的结果来倒推上一层的神经元
$$
\begin{aligned}
E_d(\overrightarrow{w}) &= \frac{1}{2}\sum \limits_{k \in output}(t_k-o_k)^2, \triangle w_{jt} = -\eta \frac{\partial E_d}{\partial w_{jt}} \\
\frac{\partial E_d}{\partial w_{jt}} &= \frac{\partial E_d}{\partial net_{j}} * \frac{\partial net_j}{\partial w_{jt}} = \frac{\partial E_d}{\partial net_{j}} * x_{jt}, net_j = \sum w_{jt}x_{jt} \\
\frac{\partial E_d}{\partial net_{j}} &= \frac{\partial E_d}{\partial o_{j}}*\frac{\partial o_j}{\partial net_{j}} = \frac{\partial }{\partial o_{j}}*\frac{1}{2}(t_j-o_j)^2\frac{\partial o_j}{\partial net_{j}} \\
&= -(t_j-o_j)\frac{\partial o_j}{\partial net_{j}}
\end{aligned}
$$

$$
\frac{\partial o_j}{\partial net_{j}} = o_j(1-o_j), 其中假设o_j=\frac{1}{1+e^{-net_j}},即为sigmod函数
$$



所以，
$$
\frac{\partial E_d}{\partial net_{j}} = -(t_j-o_j)o_j(1-o_j) \\
\triangle w_{jt} = -\eta \frac{\partial E_d}{\partial net_{j}} = \eta(t_j-o_j)o_j(1-o_j)x_{jt}
$$
上述是存在一个输出元的情形，当存在多个输出元的情况下，需要对各个输出元进行赋权。
$$
\begin{aligned}
\frac{\partial E_d}{\partial net} &= \sum \limits_{k \in Downstream(j)} \frac{\partial E_d}{\partial net_k}*\frac{\partial net_k}{\partial net_j} \\
&=\sum \limits_{...} -\delta_k \frac{\partial net_k}{\partial o_j}*\frac{\partial o_j}{\partial net_j}\\
&=\sum \limits_{...} -\delta_k w_{kj}*\frac{\partial o_j}{\partial net_j} \\
&=\sum \limits_{...} -\delta_k w_{kj}o_j(1-o_j) 
\end{aligned}
$$
其中：
$$
\delta _k = -\frac{\partial E_d}{\partial net_k},\delta_j = o_j(1-o_j)\sum \limits_{...}\delta_k w_{kj}
$$
所以，$\triangle w_{ji} = \eta \delta_j x_{ji}$。

### **BP算法的实践：**

​	1） For each output unit K , calcullate its error term $\delta_k$，
$$
\delta_k \longleftarrow o_k(1-o_k)(t_k-o_k)
$$
​	2) For each hidden unit h , calculate its error term $\delta_k$，
$$
\delta_h \longleftarrow  o_h(1-o_h)\sum \limits_{k \in outputs}w_{kh}\delta_k
$$
​	3) Update each network weight $w_{ji}$，
$$
w_{ji} \longleftarrow w_{ji} + \triangle w_{ji} = w_{ji}+\eta\delta_jx_{ji}
$$
