<div align='center' ><font size='50'>数据分析45讲</font></div>





[TOC]





# 数据分析基础篇

## 06数据分析的基本概念

###  商业智能 BI、数据仓库 DW、数据挖掘 DM 三者之间的关系

开头中的百货商店利用数据预测用户购物行为属于商业智能，他们积累的顾客的消费行为习惯会存储在数据仓库中，通过对个体进行消费行为分析总结出来的规律属于数据挖掘。

商业智能：Business Intelligence，缩写是 BI。相比于数据仓库、数据挖掘，它是一个更大的概念。商业智能可以说是基于数据仓库，经过了数据挖掘后，得到了商业价值的过程。所以说数据仓库是个金矿，数据挖掘是炼金术，而商业报告则是黄金。

数据仓库的英文是 Data Warehouse，缩写是 DW。它可以说是 BI 这个房子的地基，搭建好 DW 这个地基之后，才能进行分析使用，最后产生价值。数据仓库可以说是数据库的升级概念。从逻辑上理解，数据库和数据仓库没有什么区别，都是通过数据库技术来存储数据的。不过从数量上来讲，数据仓库的量更庞大，适用于数据挖掘和数据分析。数据库可以理解是一项技术。数据仓库将原有的多个数据来源中的数据进行汇总、整理而得。数据进入数据仓库前，必须消除数据中的不一致性，方便后续进行数据分析和挖掘。

数据挖掘的英文是 Data Mining，缩写是 DM。在商业智能 BI 中经常会使用到数据挖掘技术。数据挖掘的核心包括分类、聚类、预测、关联分析等任务，通过这些炼金术，我们可以从数据仓库中得到宝藏，比如商业报告。很多时候，企业老板总是以结果为导向，他们认为商业报告才是他们想要的，但是这也是需要经过地基 DW、搬运工 ETL、科学家 DM 等共同的努力才得到的。

###  元数据 VS 数据元

元数据（MetaData）：描述其它数据的数据，也称为“中介数据”。

数据元（Data Element）：就是最小数据单元。

在生活中，只要有一类事物，就可以定义一套元数据。举个例子，比如一本图书的信息包括了书名、作者、出版社、ISBN、出版时间、页数和定价等多个属性的信息，我们就可以把这些属性定义成一套图书的元数据。

在图书这个元数据中，书名、作者、出版社就是数据元。你可以理解是最小的数据单元。

元数据最大的好处是使信息的描述和分类实现了结构化，让机器处理起来很方便。元数据可以很方便地应用于数据仓库。比如数据仓库中有数据和数据之间的各种复杂关系，为了描述这些关系，元数据可以对数据仓库的数据进行定义，刻画数据的抽取和转换规则，存储与数据仓库主题有关的各种信息。而且整个数据仓库的运行都是基于元数据的，比如抽取调度数据、获取历史数据等。

### 数据挖掘的流程

在数据挖掘中，有几个非常重要的任务，就是分类、聚类、预测和关联分析。

1. 分类就是通过训练集得到一个分类模型，然后用这个模型可以对其他数据进行分类。这里需要说明下训练集和测试集的概念。一般来说数据可以划分为训练集和测试集。训练集是用来给机器做训练的，通常是人们整理好训练数据，以及这些数据对应的分类标识。通过训练，机器就产生了自我分类的模型，然后机器就可以拿着这个分类模型，对测试集中的数据进行分类预测。同样如果测试集中，人们已经给出了测试结果，我们就可以用测试结果来做验证，从而了解分类器在测试环境下的表现。

2.  聚类人以群分，物以类聚。聚类就是将数据自动聚类成几个类别，聚到一起的相似度大，不在一起的差异性大。我们往往利用聚类来做数据划分。

3.  预测顾名思义，就是通过当前和历史数据来预测未来趋势，它可以更好地帮助我们识别机遇和风险。

4.  关联分析就是发现数据中的关联规则，它被广泛应用在购物篮分析，或事务数据分析中。比如我们开头提到的那个案例。数据挖掘要怎么完成这些任务呢？它需要将数据库中的数据经过一系列的加工计算，最终得出有用的信息。这个过程可以用以下步骤来描述。

![img](https://static001.geekbang.org/resource/image/bb/5e/bb63e82e50de89776abf7681a5b8785e.jpg)

首先，输入我们收集到的数据，然后对数据进行预处理。预处理通常是将数据转化成我们想要的格式，然后我们再对数据进行挖掘，最后通过后处理得到我们想要的信息。

## 07用户画像：标签
### 用户画像的准则
假如你进入到一家卖羊肉串的餐饮公司，老板说现在竞争越来越激烈，要想做得好就要明白顾客喜欢什么。于是上班第一天，老板问你：“你能不能分析下用户数据，给咱们公司的业务做个赋能啊？”

首先就是将自己企业的用户画像做个白描，告诉他这些用户“都是谁”“从哪来”“要去哪”。

你可以这么和老板说：“老板啊，用户画像建模是个系统的工程，我们要解决三个问题。第一呢，就是用户从哪里来，这里我们需要统一标识用户 ID，方便我们对用户后续行为进行跟踪。我们要了解这些羊肉串的用户从哪里来，他们是为了聚餐，还是自己吃宵夜，这些场景我们都要做统计分析。第二呢，这些用户是谁？我们需要对这些用户进行标签化，方便我们对用户行为进行理解。第三呢，就是用户要到哪里去？我们要将这些用户画像与我们的业务相关联，提升我们的转化率，或者降低我们的流失率。”

![img](https://static001.geekbang.org/resource/image/0f/f4/0ff019f3f87f627dd5522411f67a45f4.jpg)

首先是进行用户的标识化，要确定该标识的唯一性。

其次对用户进行标签化，也可以叫做**用户消费行为分析**，其实也就是特征选择的过程。主要有：

​	**用户标签：它包括了性别、年龄、地域、收入、学历、职业等。这些包括了用户的基础属性**。

​	**消费标签：消费习惯、购买意向、是否对促销敏感。这些统计分析用户的消费习惯**。

​	**行为标签：时间段、频次、时长、访问路径。这些是通过分析用户行为，来得到他们使用 App 的习惯**。

​	**内容分析：对用户平时浏览的内容，尤其是停留时间长、浏览次数多的内容进行分析，分析出用户对哪些内容感兴趣，比如，金融、娱乐、教育、体育、时尚、科技等**。

可以说，用户画像是现实世界中的用户的数学建模，我们正是将海量数据进行标签化，来得到精准的用户画像，从而为企业更精准地解决问题。

最后，当你有了用户画像，可以为企业带来什么业务价值呢？我们可以从用户生命周期的三个阶段来划分业务价值，包括：**获客、粘客和留客**。

​	**获客：如何进行拉新，通过更精准的营销获取客户**。

​	**粘客：个性化推荐，搜索排序，场景运营等**。

​	**留客：流失率预测，分析关键节点降低流失率**。

如果按照数据流处理的阶段来划分用户画像建模的过程，可以分为**数据层、算法层和业务层**。你会发现在不同的层，都需要打上不同的标签。

​	**数据层指的是用户消费行为里的标签。我们可以打上“事实标签”，作为数据客观的记录**。

​	**算法层指的是透过这些行为算出的用户建模。我们可以打上“模型标签”，作为用户画像的分类标识**。

​	**业务层指的是获客、粘客、留客的手段。我们可以打上“预测标签”，作为业务关联的结果**。

所以这个标签化的流程，就是通过数据层的“事实标签”，在算法层进行计算，打上“模型标签”的分类结果，最后指导业务层，得出“预测标签”。

![img](https://static001.geekbang.org/resource/image/f0/2f/f0e94ab9319dc561df3cdd781526c22f.jpg)

### 案例：美团外卖的用户画像该如何设计？

刚才讲的是用户画像的三个阶段，以及每个阶段的准则。下面，我们来使用这些准则做个练习。

如果你是美团外卖的数据分析师，你该如何制定用户标识 ID，制定用户画像，以及基于用户画像可以做哪些业务关联？

首先，我们先回顾下美团外卖的产品背景。美团已经和大众点评进行了合并，因此在大众点评和美团外卖上都可以进行外卖下单。另外美团外卖针对的是高频 O2O 的场景，美团外卖是美团的核心产品，基本上有一半的市值都是由外卖撑起来的。

基于用户画像实施的三个阶段，我们首先需要统一用户的唯一标识，那么究竟哪个字段可以作为用户标识呢？

我们先看下美团和大众点评都是通过哪些方式登录的。

我们看到，美团采用的是手机号、微信、微博、美团账号的登录方式。大众点评采用的是手机号、微信、QQ、微博的登录方式。这里面两个 APP 共同的登录方式都是手机号、微信和微博。那么究竟哪个可以作为用户的唯一标识呢？

当然主要是以用户的注册手机号为标准。这样美团和大众点评的账号体系就可以相通。当然，大家知道在集团内部，各部门之间的协作，尤其是用户数据打通是非常困难的，所以这里建议，如果希望大数据对各个部门都能赋能，一定要在集团的战略高度上，尽早就在最开始的顶层架构上，将用户标识进行统一，这样在后续过程中才能实现用户数据的打通。

然后我们思考下，有了用户，用户画像都可以统计到哪些标签。我们按照**“用户消费行为分析”**的准则来进行设计。

用户标签：性别、年龄、家乡、居住地、收货地址、婚姻、宝宝信息、通过何种渠道进行的注册。

消费标签：餐饮口味、消费均价、团购等级、预定使用等级、排队使用等级、外卖等级。

行为标签：点外卖时间段、使用频次、平均点餐用时、访问路径。

内容分析：基于用户平时浏览的内容进行统计，包括餐饮口味、优惠敏感度等。

当你有了“用户消费行为分析”的标签之后，你就可以更好地理解业务了。比如一个经常买沙拉的人，一般很少吃夜宵。同样，一个经常吃夜宵的人，吃小龙虾的概率可能远高于其他人。这些结果都是通过数据挖掘中的关联分析得出的。

有了这些数据，我们就可以预测用户的行为。比如一个用户购买了“月子餐”后，更有可能购买婴儿水，同样婴儿相关的产品比如婴儿湿巾等的购买概率也会增大。

具体在业务层上，我们都可以基于标签产生哪些业务价值呢？

**在获客上**，我们可以找到优势的宣传渠道，如何通过个性化的宣传手段，吸引有潜在需求的用户，并刺激其转化。

**在粘客上**，如何提升用户的单价和消费频次，方法可以包括购买后的个性化推荐、针对优质用户进行优质高价商品的推荐、以及重复购买，比如通过红包、优惠等方式激励对优惠敏感的人群，提升购买频次。

**在留客上**，预测用户是否可能会从平台上流失。在营销领域，关于用户留存有一个观点——如果将顾客流失率降低 5%，公司利润将提升 25%~85%。可以看出留存率是多么的重要。用户流失可能会包括多种情况，比如用户体验、竞争对手、需求变化等，通过预测用户的流失率可以大幅降低用户留存的运营成本。

## 11数据清洗

数据清洗规则总结为以下 4 个关键点，统一起来叫**“完全合一”**。

**完整性：单条数据是否存在空值，统计的字段是否完善。**

**全面性：观察某一列的全部数值，比如在 Excel 表中，我们选中一列，可以看到该列的平均值、最大值、最小值。我们可以通过常识来判断该列是否有问题，比如：数据定义、单位标识、数值本身。**

**合法性：数据的类型、内容、大小的合法性。比如数据中存在非 ASCII 字符，性别存在了未知，年龄超过了 150 岁等。**

**唯一性：数据是否存在重复记录，因为数据通常来自不同渠道的汇总，重复的情况是常见的。行数据、列数据都需要是唯一的，比如一个人不能重复记录多次，且一个人的体重也不能在列指标中重复记录多次。**

在很多数据挖掘的教学中，数据准则通常会列出来 7~8 项，在这里我们归类成了“完全合一”4 项准则，按照以上的原则，我们能解决数据清理中遇到的大部分问题，使得数据标准、干净、连续，为后续数据统计、数据挖掘做好准备。如果想要进一步优化数据质量，还需要在实际案例中灵活使用。
#### 案例

![img](https://static001.geekbang.org/resource/image/5e/23/5e69b73b96c0d824240ac8035fe69723.png)

**1.完整性 **

**问题 1：缺失值**

在数据中有些年龄、体重数值是缺失的，这往往是因为数据量较大，在过程中，有些数值没有采集到。通常我们可以采用以下三种方法：**删除：删除数据缺失的记录；均值：使用当前列的均值；高频：使用当前列出现频率最高的数据。**

```python
df['Age'].fillna(df['Age'].mean(), inplace=True)
```
如果我们用最高频的数据进行填充，可以先通过 value_counts 获取 Age 字段最高频次 age_maxf，然后再对 Age 字段中缺失的数据用 age_maxf 进行填充：

```python
age_maxf = train_features['Age'].value_counts().index[0]
train_features['Age'].fillna(age_maxf, inplace=True)
```
**问题 2：空行**

我们发现数据中有一个空行，除了 index 之外，全部的值都是 NaN。Pandas 的 read_csv() 并没有可选参数来忽略空行，这样，我们就需要在数据被读入之后再使用 dropna() 进行处理，删除空行。

```python
df.dropna(how='all',inplace=True)
```
**2. 全面性**：

**问题：列数据的单位不统一**

观察 weight 列的数值，我们能发现 weight 列的单位不统一。有的单位是千克（kgs），有的单位是磅（lbs）。这里我使用千克作为统一的度量单位，将磅（lbs）转化为千克（kgs）：

```python

# 获取 weight 数据列中单位为 lbs 的数据
rows_with_lbs = df['weight'].str.contains('lbs').fillna(False)
print df[rows_with_lbs]
# 将 lbs转换为 kgs, 2.2lbs=1kgs
for i,lbs_row in df[rows_with_lbs].iterrows():
  # 截取从头开始到倒数第三个字符之前，即去掉lbs。
  weight = int(float(lbs_row['weight'][:-3])/2.2)
  df.at[i,'weight'] = '{}kgs'.format(weight) 
#或者用map函数,四舍五入保留2位小数，map作用于单行，apply作用于多行，applymap作用于所有元素
df['weight'] = df['weight'].map(lambda x:str(round(float(x[:-3])/2.2,2)) if x[-3:] == 'lbs' else x)
```



**3. 合理性**
**问题：非 ASCII 字符**
我们可以看到在数据集中 Firstname 和 Lastname 有一些非 ASCII 的字符。我们可以采用删除或者替换的方式来解决非 ASCII 问题，这里我们使用删除方法：

```python

# 删除非 ASCII 字符
df['first_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
df['last_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
```

**4. 唯一性**

**问题1：一列有多个参数**

在数据中不难发现，姓名列（Name）包含了两个参数 Firstname 和 Lastname。为了达到数据整洁目的，我们将 Name 列拆分成 Firstname 和 Lastname 两个字段。我们使用 Python 的 split 方法，str.split(expand=True)，将列表拆成新的列，再将原来的 Name 列删除。

```python

# 切分名字，删除源数据列
df[['first_name','last_name']] = df['name'].str.split(expand=True)
df.drop('name', axis=1, inplace=True)
```

**问题 2：重复数据**

我们校验一下数据中是否存在重复记录。如果存在重复记录，就使用 Pandas 提供的 drop_duplicates() 来删除重复数据。

```python

# 删除重复数据行
df.drop_duplicates(['first_name','last_name'],inplace=True)
```



## 12 数据集成

**数据集成的两种架构：ELT 和 ETL**

ETL 是英文 Extract、Transform 和 Load 的缩写，顾名思义它包括了数据抽取、转换、加载三个过程。ETL 可以说是进行数据挖掘这项工作前的“备菜”过程。我来解释一下数据抽取、转换、加载这三个过程。抽取是将数据从已有的数据源中提取出来。转换是对原始数据进行处理，例如将表输入 1 和 表输入 2 进行连接形成一张新的表。

![img](https://static001.geekbang.org/resource/image/31/68/31a88246298e317c90412bc9c03eee68.png)

如果是三张表连接的话，可以先将表输入 1 和表输入 2 进行连接形成表输入 1-2，然后将表输入 1-2 和表输入 3 进行连接形成新的表。然后再将生成的新表写入目的地。

![img](https://static001.geekbang.org/resource/image/bd/f7/bd39e4f480f92a1794ccc43f51acf9f7.png)

根据转换发生的顺序和位置，数据集成可以分为 ETL 和 ELT 两种架构。

ETL 的过程为提取 (Extract)——转换 (Transform)——加载 (Load)，在数据源抽取后首先进行转换，然后将转换的结果写入目的地。

ELT 的过程则是提取 (Extract)——加载 (Load)——变换 (Transform)，在抽取后将结果先写入目的地，然后利用数据库的聚合分析能力或者外部计算框架，如 Spark 来完成转换的步骤。

![img](https://static001.geekbang.org/resource/image/90/c2/90df6593871fa5974e744907bb145bc2.jpg)

目前数据集成的主流架构是 ETL，但未来使用 ELT 作为数据集成架构的将越来越多。这样做会带来多种好处：

​	ELT 和 ETL 相比，最大的区别是“重抽取和加载，轻转换”，从而可以用更轻量的方案搭建起一个数据集成平台。使用 ELT 方法，在提取完成之后，数据加载会立即开始。一方面更省时，另一方面 ELT 允许 BI 分析人员无限制地访问整个原始数据，为分析师提供了更大的灵活性，使之能更好地支持业务。

​	在 ELT 架构中，数据变换这个过程根据后续使用的情况，需要在 SQL 中进行，而不是在加载阶段进行。这样做的好处是你可以从数据源中提取数据，经过少量预处理后进行加载。这样的架构更简单，使分析人员更好地了解原始数据的变换过程。

​	几个ETL工具：kettle、

## 13 数据变换

举个例子，假设 A 考了 80 分，B 也考了 80 分，但前者是百分制，后者 500 分是满分，如果我们把从这两个渠道收集上来的数据进行集成、挖掘，就算使用效率再高的算法，结果也不是正确的。因为这两个渠道的分数代表的含义完全不同。

所以说，有时候数据变换比算法选择更重要，数据错了，算法再正确也是错的。你现在可以理解为什么 80% 的工作时间会花在前期的数据准备上了吧。那么如何让不同渠道的数据统一到一个目标数据库里呢？这样就用到了数据变换。

在数据变换前，我们需要先对字段进行筛选，然后对数据进行探索和相关性分析，接着是选择算法模型（这里暂时不需要进行模型计算），然后针对算法模型对数据的需求进行数据变换，从而完成数据挖掘前的准备工作。

![img](https://static001.geekbang.org/resource/image/90/e9/9081a928916973723e66d70c771162e9.jpg)

所以你从整个流程中可以看出，数据变换是数据准备的重要环节，它通过数据平滑、数据聚集、数据概化和规范化等方式将数据转换成适用于数据挖掘的形式。

我来介绍下这些常见的变换方法：

**1.数据平滑：**去除数据中的噪声，将连续数据离散化。这里可以采用分箱、聚类和回归的方式进行数据平滑，我会在后面给你讲解聚类和回归这两个算法；

**2.数据聚集：**对数据进行汇总，在 SQL 中有一些聚集函数可以供我们操作，比如 Max() 反馈某个字段的数值最大值，Sum() 返回某个字段的数值总和；

**3.数据概化：**将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如说上海、杭州、深圳、北京可以概化为中国。

**4.数据规范化：**使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小—最大规范化、Z—score 规范化、按小数定标规范化等，我会在后面给你讲到这些方法的使用；

**5.属性构造：**构造出新的属性并添加到属性集中。这里会用到特征工程的知识，因为通过属性与属性的连接构造新的属性，其实就是特征工程。比如说，数据表中统计每个人的英语、语文和数学成绩，你可以构造一个“总和”这个属性，来作为新属性。这样“总和”这个属性就可以用到后续的数据挖掘计算中。	在这些变换方法中，最简单易用的就是对数据进行规范化处理。下面我来给你讲下如何对数据进行规范化处理。

### 数据规范化的几种方法

**1. Min-max 规范化**

Min-max 规范化方法是将原始数据变换到[0,1]的空间中。用公式表示就是：新数值 =（原数值 - 极小值）/（极大值 - 极小值）。

**2.Z-Score 规范化**

假设 A 与 B 的考试成绩都为 80 分，A 的考卷满分是 100 分（及格 60 分），B 的考卷满分是 500 分（及格 300 分）。虽然两个人都考了 80 分，但是 A 的 80 分与 B 的 80 分代表完全不同的含义。我们定义：新数值 =（原数值 - 均值）/ 标准差。假设 A 所在的班级平均分为 80，标准差为 10。B 所在的班级平均分为 400，标准差为 100。那么 A 的新数值 =(80-80)/10=0，B 的新数值 =(80-400)/100=-3.2。那么在 Z-Score 标准下，A 的成绩会比 B 的成绩好。我们能看到 Z-Score 的优点是算法简单，不受数据量级影响，结果易于比较。不足在于，它需要数据整体的平均值和方差，而且结果没有实际意义，只是用于比较。

**3.小数定标规范化**

小数定标规范化就是通过移动小数点的位置来进行规范化。小数点移动多少位取决于属性 A 的取值中的最大绝对值。举个例子，比如属性 A 的取值范围是 -999 到 88，那么最大绝对值为 999，小数点就会移动 3 位，即新数值 = 原数值 /1000。那么 A 的取值范围就被规范化为 -0.999 到 0.088。

​	上面这三种是数值规范化中常用的几种方式。

### Python 的 SciKit-Learn 库使用

​	SciKit-Learn 是 Python 的重要机器学习库，它帮我们封装了大量的机器学习算法，比如分类、聚类、回归、降维等。此外，它还包括了数据变换模块。

**1. Min-max 规范化**

我们可以让原始数据投射到指定的空间[min, max]，在 SciKit-Learn 里有个函数 MinMaxScaler 是专门做这个的，它允许我们给定一个最大值与最小值，然后将原数据投射到[min, max]中。默认情况下[min,max]是[0,1]，也就是把原始数据投放到[0,1]范围内。我们来看下下面这个例子：

```python

# coding:utf-8
from sklearn import preprocessing
import numpy as np
# 初始化数据，每一行表示一个样本，每一列表示一个特征
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行[0,1]规范化
min_max_scaler = preprocessing.MinMaxScaler()
minmax_x = min_max_scaler.fit_transform(x)
print minmax_x
```

运行结果：

```python

[[0.         0.         0.66666667]
 [1.         1.         1.        ]
 [0.         1.         0.        ]]
```

**2. Z-Score 规范化**

​	在 SciKit-Learn 库中使用 preprocessing.scale() 函数，可以直接将给定数据进行 Z-Score 规范化。

```python

from sklearn import preprocessing
import numpy as np
# 初始化数据
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行Z-Score规范化
scaled_x = preprocessing.scale(x)
print scaled_x
```

结果：

```python

[[-0.70710678 -1.41421356  0.26726124]
 [ 1.41421356  0.70710678  1.06904497]
 [-0.70710678  0.70710678 -1.33630621]]
```

**3. 小数定标规范化**

我们需要用 NumPy 库来计算小数点的位数。NumPy 库我们之前提到过。

```python

# coding:utf-8
from sklearn import preprocessing
import numpy as np
# 初始化数据
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 小数定标规范化
j = np.ceil(np.log10(np.max(abs(x))))
scaled_x = x/(10**j)
print scaled_x
```

运行结果：

```python

[[ 0.  -0.3  0.1]
 [ 0.3  0.1  0.2]
 [ 0.   0.1 -0.1]]
```

## 14 数据可视化

​	常用的可视化视图超过 20 种，分别包括：文本表、热力图、地图、符号地图、饼图、水平条、堆叠条、并排条、树状图、圆视图、并排圆、线、双线、面积图、双组合、散点图、直方图、盒须图、甘特图、靶心图、气泡图等。

​	数据关系：

![img](https://static001.geekbang.org/resource/image/5b/d3/5bb98485841aa86f1ca18762e9e04ed3.png)

**可视化工具**

**商业智能分析**

首先在商业智能分析软件中，最著名的当属 Tableau 和 PowerBI 了，另外中国帆软出品的 FineBI 也受到国内很多企业的青睐。Tableau 是国外的商业软件，收费不低。它适合 BI 工程师、数据分析分析师。如果可以熟练掌握 Tableau，那么找到一份数据分析的工作是不难的。

PowerBI 是微软出品的，可以和 Excel 搭配使用，你可以通过 PowerBI 来呈现 Excel 的可视化内容。在 BI 行业中，Tableau 和 PowerBI 远超其他 BI 产品，处于行业的领导者的位置。另外 FineBI 是中国的帆软出品，针对国内使用更加友好，同时也倾向于企业级应用的 BI。

**可视化大屏类**

大屏作为一种视觉效果强、科技感强的技术，被企业老板所青睐，可以很好地展示公司的数据化能力。这里给你介绍两款可视化大屏的软件 DataV 和 FineReport。

**DataV** 是一款可视化的工具，天猫双十一大屏就是用它呈现的。你要做的就是选择相应的控件，配置控件的样式、数据传输和交互效果等。当然 DataV 本身有一些免费的模板，你可以直接通过模板来创建。不过一些特殊的控件和交互效果还是需要购买企业版才行。

**FineReport**是帆软出品的工具，你可以看出他家的产品基本上都是 Fine 开头的，包括刚才给你介绍的 FineBI。FineReport 可以做数据大屏，也可以做可视化报表，在很多行业都有解决方案，操作起来也很方便。可以实时连接业务数据，对数据进行展示。

![img](https://static001.geekbang.org/resource/image/bd/5b/bd49dbaffdc170ecc4d56d946afd5c5b.jpg)

## 15 python数据可视化的10种功能

### 可视化视图类型

按照数据之间的关系，我们可以把可视化视图划分为 4 类，它们分别是**比较、联系、构成和分布**。

**趋势比较：**比较数据间各类别的关系，或者是它们随着时间的变化趋势，比如折线图；

**联系：**查看两个或两个以上变量之间的关系，比如散点图；

**构成：**每个部分占整体的百分比，或者是随着时间的百分比变化，比如饼图；

**分布：**关注单个变量，或者多个变量的分布情况，比如直方图。

同样，按照变量的个数，我们可以把可视化视图划分为**单变量分析和多变量分析**。

**单变量分析**指的是一次只关注一个变量。比如我们只关注“身高”这个变量，来看身高的取值分布，而暂时忽略其他变量。

**多变量分析**可以让你在一张图上可以查看两个以上变量的关系。比如“身高”和“年龄”，你可以理解是同一个人的两个参数，这样在同一张图中可以看到每个人的“身高”和“年龄”的取值，从而分析出来这两个变量之间是否存在某种联系。

可视化的视图可以说是分门别类，多种多样，今天我主要介绍常用的 10 种视图，这些视图包括了散点图、折线图、直方图、条形图、箱线图、饼图、热力图、蜘蛛图、二元变量分布和成对关系。

![img](https://static001.geekbang.org/resource/image/46/75/4673a17085302cfe9177f8ee687ac675.png)

**散点图**

散点图的英文叫做 scatter plot，它将两个变量的值显示在二维坐标中，非常适合展示两个变量之间的关系。当然，除了二维的散点图，我们还有三维的散点图。

Matplotlib中需要使用 plt.scatter(x, y, marker=None) 函数。x、y 是坐标，marker 代表了标记的符号。比如“x”、“>”或者“o”。

 seaborn中使用sns.jointplot(x, y, data=None, kind='scatter') 函数。其中 x、y 是 data 中的下标。data 就是我们要传入的数据，一般是 DataFrame 类型。kind 这类我们取 scatter，代表散点的意思。

模拟：

```python

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
N = 1000
x = np.random.randn(N)
y = np.random.randn(N)
# 用Matplotlib画散点图
plt.scatter(x, y,marker='x')
plt.show()
# 用Seaborn画散点图
df = pd.DataFrame({'x': x, 'y': y})
sns.jointplot(x="x", y="y", data=df, kind='scatter');
plt.show()
```

**Matplotlib 绘制：**

![img](https://static001.geekbang.org/resource/image/28/03/2823ea9c7c2d988c1fdb3e7c8fb1e603.png)

**Seaborn 绘制：**

![img](https://static001.geekbang.org/resource/image/5f/b9/5f06e23188cb31bc549cfd60696e75b9.png)

**折线图**

在 Matplotlib 中，我们可以直接使用 plt.plot() 函数，当然需要提前把数据按照 x 轴的大小进行排序，要不画出来的折线图就无法按照 x 轴递增的顺序展示。

在 Seaborn 中，我们使用 sns.lineplot (x, y, data=None) 函数。其中 x、y 是 data 中的下标。data 就是我们要传入的数据，一般是 DataFrame 类型。

```python

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
x = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]
y = [5, 3, 6, 20, 17, 16, 19, 30, 32, 35]
# 使用Matplotlib画折线图
plt.plot(x, y)
plt.show()
# 使用Seaborn画折线图
df = pd.DataFrame({'x': x, 'y': y})
sns.lineplot(x="x", y="y", data=df)
plt.show()
```

结果相同

![img](https://static001.geekbang.org/resource/image/25/88/258c6a2fbd7786ed7bd86a5f50c49b88.png)

**直方图**

在 Matplotlib 中，我们使用 plt.hist(x, bins=10) 函数，其中参数 x 是一维数组，bins 代表直方图中的箱子数量，默认是 10。

在 Seaborn 中，我们使用 sns.distplot(x, bins=10, kde=True) 函数。其中参数 x 是一维数组，bins 代表直方图中的箱子数量，kde 代表显示核密度估计，默认是 True，我们也可以把 kde 设置为 False，不进行显示。核密度估计是通过核函数帮我们来估计概率密度的方法。

```python

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
a = np.random.randn(100)
s = pd.Series(a) 
# 用Matplotlib画直方图
plt.hist(s)
plt.show()
# 用Seaborn画直方图
sns.distplot(s, kde=False)
plt.show()
sns.distplot(s, kde=True)
plt.show()
```

**条形图**

在 Matplotlib 中，我们使用 plt.bar(x, height) 函数，其中参数 x 代表 x 轴的位置序列，height 是 y 轴的数值序列，也就是柱子的高度。

在 Seaborn 中，我们使用 sns.barplot(x=None, y=None, data=None) 函数。其中参数 data 为 DataFrame 类型，x、y 是 data 中的变量。

```python

import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
x = ['Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5']
y = [5, 4, 8, 12, 7]
# 用Matplotlib画条形图
plt.bar(x, y)
plt.show()
# 用Seaborn画条形图
sns.barplot(x, y)
plt.show()
```

**箱线图**

在 Matplotlib 中，我们使用 plt.boxplot(x, labels=None) 函数，其中参数 x 代表要绘制箱线图的数据，labels 是缺省值，可以为箱线图添加标签。

在 Seaborn 中，我们使用 sns.boxplot(x=None, y=None, data=None) 函数。其中参数 data 为 DataFrame 类型，x、y 是 data 中的变量。

```python

# 数据准备
# 生成10*4维度数据
data=np.random.normal(size=(10,4)) 
labels = ['A','B','C','D']
# 用Matplotlib画箱线图
plt.boxplot(data,labels=labels)
plt.show()
# 用Seaborn画箱线图
df = pd.DataFrame(data, columns=labels)
sns.boxplot(data=df)
plt.show()
```

**饼图**

在 Matplotlib 中，我们使用 plt.pie(x, labels=None) 函数，其中参数 x 代表要绘制饼图的数据，labels 是缺省值，可以为饼图添加标签。这里我设置了 labels 数组，分别代表高中、本科、硕士、博士和其他几种学历的分类标签。nums 代表这些学历对应的人数。

```python

import matplotlib.pyplot as plt
# 数据准备
nums = [25, 37, 33, 37, 6]
labels = ['High-school','Bachelor','Master','Ph.d', 'Others']
# 用Matplotlib画饼图
plt.pie(x = nums, labels=labels)
plt.show()
```

**热力图**

一般使用 Seaborn 中的 sns.heatmap(data) 函数，其中 data 代表需要绘制的热力图数据。这里我们使用 Seaborn 中自带的数据集 flights，该数据集记录了 1949 年到 1960 年期间，每个月的航班乘客的数量。

```python

import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
flights = sns.load_dataset("flights")
data=flights.pivot('year','month','passengers')
# 用Seaborn画热力图
sns.heatmap(data)
plt.show()
```

![img](https://static001.geekbang.org/resource/image/57/93/57e1bc17d943620620fb087d6190df93.png)

**蜘蛛图**

使用 Matplotlib 来进行画图，首先设置两个数组：labels 和 stats。他们分别保存了这些属性的名称和属性值。

因为蜘蛛图是一个圆形，你需要计算每个坐标的角度，然后对这些数值进行设置。当画完最后一个点后，需要与第一个点进行连线。因为需要计算角度，所以我们要准备 angles 数组；又因为需要设定统计结果的数值，所以我们要设定 stats 数组。并且需要在原有 angles 和 stats 数组上增加一位，也就是添加数组的第一个元素。

```python

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.font_manager import FontProperties  
# 数据准备
labels=np.array([u"推进","KDA",u"生存",u"团战",u"发育",u"输出"])
stats=[83, 61, 95, 67, 76, 88]
# 画图数据准备，角度、状态值
angles=np.linspace(0, 2*np.pi, len(labels), endpoint=False)
stats=np.concatenate((stats,[stats[0]]))
angles=np.concatenate((angles,[angles[0]]))
# 用Matplotlib画蜘蛛图
fig = plt.figure()
ax = fig.add_subplot(111, polar=True)   
ax.plot(angles, stats, 'o-', linewidth=2)
ax.fill(angles, stats, alpha=0.25)
# 设置中文字体
font = FontProperties(fname=r"C:\Windows\Fonts\simhei.ttf", size=14)  
ax.set_thetagrids(angles * 180/np.pi, labels, FontProperties=font)
plt.show()
```

![img](https://static001.geekbang.org/resource/image/19/7d/1924d3cbf035053fa3d5043794624c7d.png)

**二元变量分布**

在 Seaborn 里，使用 sns.jointplot(x, y, data=None, kind) 函数即可。其中用 kind 表示不同的视图类型：“kind='scatter'”代表散点图，“kind='kde'”代表核密度图，“kind='hex' ”代表 Hexbin 图，它代表的是直方图的二维模拟。

这里我们使用 Seaborn 中自带的数据集 tips，这个数据集记录了不同顾客在餐厅的消费账单及小费情况。代码中 total_bill 保存了客户的账单金额，tip 是该客户给出的小费金额。我们可以用 Seaborn 中的 jointplot 来探索这两个变量之间的关系。

```python

import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
tips = sns.load_dataset("tips")
print(tips.head(10))
# 用Seaborn画二元变量分布图（散点图，核密度图，Hexbin图）
sns.jointplot(x="total_bill", y="tip", data=tips, kind='scatter')
sns.jointplot(x="total_bill", y="tip", data=tips, kind='kde')
sns.jointplot(x="total_bill", y="tip", data=tips, kind='hex')
plt.show()
```

**成对关系**

直接采用 sns.pairplot() 函数。它会同时展示出 DataFrame 中每对变量的关系，另外在对角线上，你能看到每个变量自身作为单变量的分布情况。

这里我们使用 Seaborn 中自带的 iris 数据集，这个数据集也叫鸢尾花数据集。鸢尾花可以分成 Setosa、Versicolour 和 Virginica 三个品种，在这个数据集中，针对每一个品种，都有 50 个数据，每个数据中包括了 4 个属性，分别是花萼长度、花萼宽度、花瓣长度和花瓣宽度。通过这些数据，需要你来预测鸢尾花卉属于三个品种中的哪一种。

```python

import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
iris = sns.load_dataset('iris')
# 用Seaborn画成对关系
sns.pairplot(iris)
plt.show()
```

![img](https://static001.geekbang.org/resource/image/88/0d/885450d23f468b9cbcabd90ff9a3480d.png)

![img](https://static001.geekbang.org/resource/image/8e/d2/8ed2addb00a4329dd63bba669f427fd2.png)

# 数据分析算法篇

此处内容放在数据挖掘理论与算法文档  [数据挖掘理论与算法](C:\Users\Lenovo\Desktop\学习\自学\数据挖掘理论与算法.md)

# 数据分析实战篇

## 37、自动化运营微博（selenium）

定位一个元素，可以通过 id、name、class、tag、链接上的全部文本、链接上的部分文本、XPath 或者 CSS 进行定位，在 Selenium Webdriver 中也提供了这 8 种方法方便我们定位元素。

1.通过 id 定位：我们可以使用 find_element_by_id() 函数。比如我们想定位 id=loginName 的元素，就可以使用 browser.find_element_by_id(“loginName”)。

2.通过 name 定位：我们可以使用 find_element_by_name() 函数，比如我们想要对 name=key_word 的元素进行定位，就可以使用 browser.find_element_by_name(“key_word”)。

3.通过 class 定位：可以使用 find_element_by_class_name() 函数。

4.通过 tag 定位：使用 find_element_by_tag_name() 函数。

5.通过 link 上的完整文本定位：使用 find_element_by_link_text() 函数。

6.通过 link 上的部分文本定位：使用 find_element_by_partial_link_text() 函数。有时候超链接上的文本很长，我们通过查找部分文本内容就可以定位。通过 

7.XPath 定位：使用 find_element_by_xpath() 函数。使用 XPath 定位的通用性比较好，因为当 id、name、class 为多个，或者元素没有这些属性值的时候，XPath 定位可以帮我们完成任务。

8.通过 CSS 定位：使用 find_element_by_css_selector() 函数。CSS 定位也是常用的定位方法，相比于 XPath 来说更简洁。

在我们获取某个元素之后，就可以对这个元素进行操作了，对元素进行的操作包括：

1.清空输入框的内容：使用 clear() 函数；

2.在输入框中输入内容：使用 send_keys(content) 函数传入要输入的文本；

3.点击按钮：使用 click() 函数，如果元素是个按钮或者链接的时候，可以点击操作；

4.提交表单：使用 submit() 函数，元素对象为一个表单的时候，可以提交表单；

```python

from selenium import webdriver
import time
browser = webdriver.Chrome()
# 登录微博
def weibo_login(username, password):
     # 打开微博登录页
     browser.get('https://passport.weibo.cn/signin/login')
     browser.implicitly_wait(5)
     time.sleep(1)
     # 填写登录信息：用户名、密码
     browser.find_element_by_id("loginName").send_keys(username)
     browser.find_element_by_id("loginPassword").send_keys(password)
     time.sleep(1)
     # 点击登录
     browser.find_element_by_id("loginAction").click()
     time.sleep(1)
# 设置用户名、密码，需要明文填写
username = 'XXXX'
password = "XXXX"
weibo_login(username, password)
```

通过 HTML 标签分析，我们能看到 id=loginName 和 id=loginPassword 对应着用户名和密码的输入框，获取这两个元素之后，使用 send_keys() 设置用户名和密码，然后通过 id=loginAction 找到登录按钮，获取该元素，使用 click() 函数点击提交。

**微博自动化运营：加关注，写评论，发微博**

```python
#加关注
# 添加指定的用户
def add_follow(uid):
    browser.get('https://m.weibo.com/u/'+str(uid))
    time.sleep(1)
    #browser.find_element_by_id("follow").click()
    follow_button = browser.find_element_by_xpath('//div[@class="m-add-box m-followBtn"]')
    follow_button.click()
    time.sleep(1)
    # 选择分组
    group_button = browser.find_element_by_xpath('//div[@class="m-btn m-btn-white m-btn-text-black"]')
    group_button.click()
    time.sleep(1)
# 每天学点心理学UID
uid = '1890826225' 
add_follow(uid)
```

这里有两点你需要注意。

第一点是如何找到用户的 UID 呢？

在微博中，用户是用 UID 做唯一标识的。代码中我随机指定了一个 UID，你也可以指定其他的 UID。那么如何获取用户的 UID 的呢？你可以点击任何一个微博用户，查看他的 URL 链接，比如链接是https://weibo.com/u/5020181423 ，那么 u 后面的数字 5020181423 即为用户的 UID。你也可能遇到https://weibo.com/ixinli（每天学点心理学）这样的链接情况，那么通过查看他的粉丝即可以显示出 UID，比如这个微博的粉丝链接是https://weibo.com/1890826225/fans，那么 UID 对应的就是 1890826225。

第二个需要注意的是使用 XPath 定位元素定位。我们如何找到“关注”这个按钮的元素标识呢？在 Chrome 浏览器中，在“关注”按钮用鼠标右键点击，选择“检查”查看这个元素对应的代码。通过分析，你能看到这个元素的 div 标签中的 class 属性为 m-add-box m-followBtn，那么你可以通过 find_element_by_xpath(’//div[@class=“m-add-box m-followBtn”]’) 获取这个元素。关注之后，程序会弹出选择分组的页面，如下所示。

通过同样的方法，你可以查看“取消”这个按钮对应的 HTML 标签特征，然后通过 find_element_by_xpath 定位，使用 click() 函数提交。这样我们就关注了一个指定的用户。

```python
#
# 给指定某条微博添加内容
def add_comment(weibo_url, content):
    browser.get(weibo_url)
    browser.implicitly_wait(5)
    content_textarea = browser.find_element_by_css_selector("textarea.W_input").clear()
    content_textarea = browser.find_element_by_css_selector("textarea.W_input").send_keys(content)
    time.sleep(2)
    comment_button = browser.find_element_by_css_selector(".W_btn_a").click()
    time.sleep(1)
 
# 发文字微博
def post_weibo(content):
    # 跳转到用户的首页
    browser.get('https://weibo.com')
    browser.implicitly_wait(5)
    # 点击右上角的发布按钮
    post_button = browser.find_element_by_css_selector("[node-type='publish']").click()
    # 在弹出的文本框中输入内容
    content_textarea = browser.find_element_by_css_selector("textarea.W_input").send_keys(content)
    time.sleep(2)
    # 点击发布按钮
    post_button = browser.find_element_by_css_selector("[node-type='submit']").click()
    time.sleep(1)
# 给指定的微博写评论
weibo_url = 'https://weibo.com/1890826225/HjjqSahwl'
content = 'Gook Luck!好运已上路！'
# 自动发微博
content = '每天学点心理学'
post_weibo(content)
```

这里需要关注的问题是：

1.找到某条微博的链接，可以在某个用户微博页面中点击时间的链接，这样就可以获取这条微博的链接。

2.定位评论区中输入框元素的位置,将鼠标移动到评论框中，在 Chrome 浏览器中点击右键“查看”，可以看到这个 textarea 的 class 是 W_input，使用 find_element_by_css_selector 函数进行定位，然后通过 send_keys 设置内容。最后就是发微博的流程。我们可以观察到点击微博页面的右上角的按钮后，会弹出一个发微博的文本框，设置好内容，点击发送即可。发微博的函数和写评论的代码类似，使用 find_element_by_css_selector() 函数定位，通过 send_keys() 设置内容的设置，最后通过 click() 发送。

![img](https://static001.geekbang.org/resource/image/33/d5/33ee64c5a434e1a7093594499e9c05d5.png)



## 38 数据可视化实战：给毛不易的歌词做词云展示

词云也叫文字云，它帮助我们统计文本中高频出现的词，过滤掉某些常用词（比如“作曲”“作词”），将文本中的重要关键词进行可视化，方便分析者更好更快地了解文本的重点

```python

wc = WordCloud(
    background_color='white',# 设置背景颜色
    mask=backgroud_Image,# 设置背景图片
    font_path='./SimHei.ttf',  # 设置字体，针对中文的情况需要设置中文字体，否则显示乱码
    max_words=100, # 设置最大的字数
    stopwords=STOPWORDS,# 设置停用词
  max_font_size=150,# 设置字体最大值
  width=2000,# 设置画布的宽度
  height=1200,# 设置画布的高度
    random_state=30# 设置多少种随机状态，即多少种颜色
)

wordcloud = wc.generate(text)


wordcloud.to_file('xx.jpg')
#或
import matplotlib.pyplot as plt
plt.imshow(wordcloud)
plt.axis("off")
plt.show()
```

```python

#-*- coding:utf-8 -*-
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import jieba
from PIL import Image
import numpy as np
# 生成词云
def create_word_cloud(f):
     print('根据词频计算词云')
     text = " ".join(jieba.cut(f,cut_all=False, HMM=True))
     wc = WordCloud(
           font_path="./SimHei.ttf",
           max_words=100,
           width=2000,
           height=1200,
    )

# 去掉停用词
def remove_stop_words(f):
     stop_words = ['学会', '就是', '什么']
     for stop_word in stop_words:
           f = f.replace(stop_word, '')
     return f


     wordcloud = wc.generate(text)
     # 写词云图片
     wordcloud.to_file("wordcloud.jpg")
     # 显示词云文件
     plt.imshow(wordcloud)
     plt.axis("off")
     plt.show()

f = '数据分析全景图及修炼指南\
学习数据挖掘的最佳学习路径是什么？\
Python基础语法：开始你的Python之旅\
Python科学计算：NumPy\
Python科学计算：Pandas\
学习数据分析要掌握哪些基本概念？\
用户画像：标签化就是数据的抽象能力\
数据采集：如何自动化采集数据？\
数据采集：如何用八爪鱼采集微博上的“D&G”评论？\
Python爬虫：如何自动化下载王祖贤海报？\
数据清洗：数据科学家80%时间都花费在了这里？\
数据集成：这些大号一共20亿粉丝？\
数据变换：大学成绩要求正态分布合理么？\
数据可视化：掌握数据领域的万金油技能\
一次学会Python数据可视化的10种技能'
```

**给毛不易的歌词做个词云**

![img](https://static001.geekbang.org/resource/image/7c/97/7cff33b392cec653ca2e68fbecd4ef97.jpg)

```python

# -*- coding:utf-8 -*-
# 网易云音乐 通过歌手ID，生成该歌手的词云
import requests
import sys
import re
import os
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import jieba
from PIL import Image
import numpy as np
from lxml import etree
 
headers = {
       'Referer'  :'http://music.163.com',
       'Host'     :'music.163.com',
       'Accept'   :'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
       'User-Agent':'Chrome/10'
    }
 
# 得到某一首歌的歌词
def get_song_lyric(headers,lyric_url):
    res = requests.request('GET', lyric_url, headers=headers)
    if 'lrc' in res.json():
       lyric = res.json()['lrc']['lyric']
       new_lyric = re.sub(r'[\d:.[\]]','',lyric)
       return new_lyric
    else:
       return ''
       print(res.json())
# 去掉停用词
def remove_stop_words(f):
    stop_words = ['作词', '作曲', '编曲', 'Arranger', '录音', '混音', '人声', 'Vocal', '弦乐', 'Keyboard', '键盘', '编辑', '助理', 'Assistants', 'Mixing', 'Editing', 'Recording', '音乐', '制作', 'Producer', '发行', 'produced', 'and', 'distributed']
    for stop_word in stop_words:
       f = f.replace(stop_word, '')
    return f
# 生成词云
def create_word_cloud(f):
    print('根据词频，开始生成词云!')
    f = remove_stop_words(f)
    cut_text = " ".join(jieba.cut(f,cut_all=False, HMM=True))
    wc = WordCloud(
       font_path="./wc.ttf",
       max_words=100,
       width=2000,
       height=1200,
    )
    print(cut_text)
    wordcloud = wc.generate(cut_text)
    # 写词云图片
    wordcloud.to_file("wordcloud.jpg")
    # 显示词云文件
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.show()
# 得到指定歌手页面 热门前50的歌曲ID，歌曲名
def get_songs(artist_id):
    page_url = 'https://music.163.com/artist?id=' + artist_id
    # 获取网页HTML
    res = requests.request('GET', page_url, headers=headers)
    # 用XPath解析 前50首热门歌曲
    html = etree.HTML(res.text)
    href_xpath = "//*[@id='hotsong-list']//a/@href"
    name_xpath = "//*[@id='hotsong-list']//a/text()"
    hrefs = html.xpath(href_xpath)
    names = html.xpath(name_xpath)
    # 设置热门歌曲的ID，歌曲名称
    song_ids = []
    song_names = []
    for href, name in zip(hrefs, names):
       song_ids.append(href[9:])
       song_names.append(name)
       print(href, '  ', name)
    return song_ids, song_names
# 设置歌手ID，毛不易为12138269
artist_id = '12138269'
[song_ids, song_names] = get_songs(artist_id)
# 所有歌词
all_word = ''
# 获取每首歌歌词
for (song_id, song_name) in zip(song_ids, song_names):
    # 歌词API URL
    lyric_url = 'http://music.163.com/api/song/lyric?os=pc&id=' + song_id + '&lv=-1&kv=-1&tv=-1'
    lyric = get_song_lyric(headers, lyric_url)
    all_word = all_word + ' ' + lyric
    print(song_name)
#根据词频 生成词云
create_word_cloud(all_word)
```

**模块说明：**

首先我编写 get_songs 函数，可以得到指定歌手页面中热门前 50 的歌曲 ID，歌曲名。在这个函数里，我使用 requests.request 函数获取毛不易歌手页面的 HTML。这里需要传入指定的请求头（headers），否则获取不到完整的信息。然后用 XPath 解析并获取指定的内容，这个模块中，我想获取的是歌曲的链接和名称。

其中歌曲的链接类似 /song?id=536099160 这种形式，你能看到字符串第 9 位之后，就是歌曲的 ID。

有关 XPath 解析的内容，我在第 10 节讲到过，如果你忘记了，可以看一下。一般来说，XPath 解析 99% 的可能都是以 // 开头，因为你需要获取所有符合这个 XPath 的内容。我们通过分析 HTML 代码，能看到一个关键的部分：id=‘hotsong-list’。这个代表热门歌曲列表，也正是我们想要解析的内容。我们想要获取这个热门歌曲列表下面所有的链接，XPath 解析就可以写成 //*[@id=‘hotsong-list’]//a。然后你能看到歌曲链接是 href 属性，歌曲名称是这个链接的文本。

获得歌曲 ID 之后，我们还需要知道这个歌曲的歌词，对应代码中的 get_song_lyric 函数，在这个函数里调用了网易云的歌词 API 接口，比如http://music.163.com/api/song/lyric?os=pc&id=536099160&lv=-1&kv=-1&tv=-1。你

能看到歌词文件里面还保存了时间信息，我们需要去掉这部分。因此我使用了 re.sub 函数，通过正则表达式匹配，将[]中数字信息去掉，方法为 re.sub(r’[\d:.[]]’,’’,lyric)。最后我们还需要设置一些歌词中常用的停用词，比如“作词”“作曲”“编曲”等，编写 remove_stop_words 函数，将歌词文本中的停用词去掉。最后编写 create_word_cloud 函数，通过歌词文本生成词云文件

![img](https://static001.geekbang.org/resource/image/0c/6d/0cbc5f3e4ecd41af9a872fd9b4aed06d.png)



## 39 数据挖掘实战（1）：信用卡违约率分析

数据挖掘的过程中，我们经常会遇到一些问题，比如：如何选择各种分类器，到底选择哪个分类算法，是 SVM，决策树，还是 KNN？如何优化分类器的参数，以便得到更好的分类准确率？

**目标：**

1.创建各种分类器，包括已经掌握的 SVM、决策树、KNN 分类器，以及随机森林分类器；

2.掌握 GridSearchCV 工具，优化算法模型的参数；

3.使用 Pipeline 管道机制进行流水线作业。因为在做分类之前，我们还需要一些准备过程，比如数据规范化，或者数据降维等。

**构建随机森林分类器**

随机森林的英文是 Random Forest，英文简写是 RF。它实际上是一个包含多个决策树的分类器，每一个子分类器都是一棵 CART 分类回归树。所以随机森林既可以做分类，又可以做回归。当它做分类的时候，输出结果是每个子分类器的分类结果中最多的那个。你可以理解是每个分类器都做投票，取投票最多的那个结果。当它做回归的时候，输出结果是每棵 CART 树的回归结果的平均值。

![img](https://static001.geekbang.org/resource/image/35/f9/352035fe3e92d412d652fd55c77f23f9.png)

**使用 GridSearchCV 工具对模型参数进行调优**

Python 提供了一个很好用的工具 GridSearchCV，它是 Python 的参数自动搜索模块。只要告诉它想要调优的参数有哪些以及参数的取值范围，它就会把所有的情况都跑一遍，然后告诉我们哪个参数是最优的，结果如何。

```python
from sklearn.model_selection import GridSearchCV
```

![img](https://static001.geekbang.org/resource/image/70/fd/7042cb450e5dcac9306d0178265642fd.png)

构造完 GridSearchCV 之后，我们就可以使用 fit 函数拟合训练，使用 predict 函数预测，这时预测采用的是最优参数情况下的分类器。这里举一个简单的例子，我们用 sklearn 自带的 IRIS 数据集，采用随机森林对 IRIS 数据分类。假设我们想知道 n_estimators 在 1-10 的范围内取哪个值的分类结果最好，可以编写代码：

```python
# -*- coding: utf-8 -*-
# 使用RandomForest对IRIS数据集进行分类
# 利用GridSearchCV寻找最优参数
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import load_iris
rf = RandomForestClassifier()
parameters = {"n_estimators": range(1,11)}
iris = load_iris()
# 使用GridSearchCV进行参数调优
clf = GridSearchCV(estimator=rf, param_grid=parameters)
# 对iris数据集进行分类
clf.fit(iris.data, iris.target)
print("最优分数： %.4lf" %clf.best_score_)
print("最优参数：", clf.best_params_)
运行结果如下：
最优分数： 0.9667
最优参数： {'n_estimators': 6}
```

**使用 Pipeline 管道机制进行流水线作业**

做分类的时候往往都是有步骤的，比如先对数据进行规范化处理，你也可以用 PCA 方法（一种常用的降维方法）对数据降维，最后使用分类器分类。

Python 有一种 Pipeline 管道机制。管道机制就是让我们把每一步都按顺序列下来，从而创建 Pipeline 流水线作业。每一步都采用 (‘名称’, 步骤) 的方式来表示。

我们需要先采用 StandardScaler 方法对数据规范化，即采用数据规范化为均值为 0，方差为 1 的正态分布，然后采用 PCA 方法对数据进行降维，最后采用随机森林进行分类。具体代码如下：

```python
from sklearn.model_selection import GridSearchCV
pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('pca', PCA()),
        ('randomforestclassifier', RandomForestClassifier())
])
```

那么我们现在采用 Pipeline 管道机制，用随机森林对 IRIS 数据集做一下分类。先用 StandardScaler 方法对数据规范化，然后再用随机森林分类，编写代码如下：

```python
# -*- coding: utf-8 -*-
# 使用RandomForest对IRIS数据集进行分类
# 利用GridSearchCV寻找最优参数,使用Pipeline进行流水作业
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
rf = RandomForestClassifier()
parameters = {"randomforestclassifier__n_estimators": range(1,11)}
iris = load_iris()
pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('randomforestclassifier', rf)
])
# 使用GridSearchCV进行参数调优
clf = GridSearchCV(estimator=pipeline, param_grid=parameters)
# 对iris数据集进行分类
clf.fit(iris.data, iris.target)
print("最优分数： %.4lf" %clf.best_score_)
print("最优参数：", clf.best_params_)
运行结果：
最优分数： 0.9667
最优参数： {'randomforestclassifier__n_estimators': 9}
```

**对信用卡违约率进行分析**

数据集你可以从 GitHub 上下载：https://github.com/cystanford/credit_default。

![img](https://static001.geekbang.org/resource/image/17/88/1730fb3a809c99950739e7f50e1a6988.jpg)

1. 加载数据；

2. 准备阶段：探索数据，采用数据可视化方式可以让我们对数据有更直观的了解，比如我们想要了解信用卡违约率和不违约率的人数。因为数据集没有专门的测试集，我们还需要使用 train_test_split 划分数据集。

3. 分类阶段：之所以把数据规范化放到这个阶段，是因为我们可以使用 Pipeline 管道机制，将数据规范化设置为第一步，分类为第二步。因为我们不知道采用哪个分类器效果好，所以我们需要多用几个分类器，比如 SVM、决策树、随机森林和 KNN。然后通过 GridSearchCV 工具，找到每个分类器的最优参数和最优分数，最终找到最适合这个项目的分类器和该分类器的参数。

   基于上面的流程，具体代码如下：

```python
# -*- coding: utf-8 -*-
# 信用卡违约率分析
import pandas as pd
from sklearn.model_selection import learning_curve, train_test_split,GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from matplotlib import pyplot as plt
import seaborn as sns
# 数据加载
data = data = pd.read_csv('./UCI_Credit_Card.csv')
# 数据探索
print(data.shape) # 查看数据集大小
print(data.describe()) # 数据集概览
# 查看下一个月违约率的情况
next_month = data['default.payment.next.month'].value_counts()
print(next_month)
df = pd.DataFrame({'default.payment.next.month': next_month.index,'values': next_month.values})
plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签
plt.figure(figsize = (6,6))
plt.title('信用卡违约率客户\n (违约：1，守约：0)')
sns.set_color_codes("pastel")
sns.barplot(x = 'default.payment.next.month', y="values", data=df)
locs, labels = plt.xticks()
plt.show()
# 特征选择，去掉ID字段、最后一个结果字段即可
data.drop(['ID'], inplace=True, axis =1) #ID这个字段没有用
target = data['default.payment.next.month'].values
columns = data.columns.tolist()
columns.remove('default.payment.next.month')
features = data[columns].values
# 30%作为测试集，其余作为训练集
train_x, test_x, train_y, test_y = train_test_split(features, target, test_size=0.30, stratify = target, random_state = 1)
    
# 构造各种分类器
classifiers = [
    SVC(random_state = 1, kernel = 'rbf'),    
    DecisionTreeClassifier(random_state = 1, criterion = 'gini'),
    RandomForestClassifier(random_state = 1, criterion = 'gini'),
    KNeighborsClassifier(metric = 'minkowski'),
]
# 分类器名称
classifier_names = [
            'svc', 
            'decisiontreeclassifier',
            'randomforestclassifier',
            'kneighborsclassifier',
]
# 分类器参数
classifier_param_grid = [
            {'svc__C':[1], 'svc__gamma':[0.01]},
            {'decisiontreeclassifier__max_depth':[6,9,11]},
            {'randomforestclassifier__n_estimators':[3,5,6]} ,
            {'kneighborsclassifier__n_neighbors':[4,6,8]},
]
 
# 对具体的分类器进行GridSearchCV参数调优
def GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, param_grid, score = 'accuracy'):
    response = {}
    gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, scoring = score)
    # 寻找最优的参数 和最优的准确率分数
    search = gridsearch.fit(train_x, train_y)
    print("GridSearch最优参数：", search.best_params_)
    print("GridSearch最优分数： %0.4lf" %search.best_score_)
  predict_y = gridsearch.predict(test_x)
    print("准确率 %0.4lf" %accuracy_score(test_y, predict_y))
    response['predict_y'] = predict_y
    response['accuracy_score'] = accuracy_score(test_y,predict_y)
    return response
 
for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid):
    pipeline = Pipeline([
            ('scaler', StandardScaler()),
            (model_name, model)
    ])
    result = GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, model_param_grid , score = 'accuracy')
```

![img](https://static001.geekbang.org/resource/image/14/16/14f9cddc17d6cceb0b8cbc4381c65216.png)



## 40 数据挖掘实战（2）：信用卡诈骗分析

相比于信用卡违约的比例，信用卡欺诈的比例更小，但是危害极大。如何通过以往的交易数据分析出每笔交易是否正常，是否存在盗刷风险是我们这次项目的目标。

通过今天的学习，你需要掌握以下几个方面：

1. 了解逻辑回归分类，以及如何在 sklearn 中使用它；
2. 信用卡欺诈属于二分类问题，欺诈交易在所有交易中的比例很小，对于这种数据不平衡的情况，到底采用什么样的模型评估标准会更准确；
3. 完成信用卡欺诈分析的实战项目，并通过数据可视化对数据探索和模型结果评估进一步加强了解。

**构建逻辑回归分类器**

在 sklearn 中，我们使用 LogisticRegression() 函数构建逻辑回归分类器，函数里有一些常用的构造参数：

1. penalty：惩罚项，取值为 l1 或 l2，默认为 l2。当模型参数满足高斯分布的时候，使用 l2，当模型参数满足拉普拉斯分布的时候，使用 l1；

2. solver：代表的是逻辑回归损失函数的优化方法。有 5 个参数可选，分别为 liblinear、lbfgs、newton-cg、sag 和 saga。默认为 liblinear，适用于数据量小的数据集，当数据量大的时候可以选用 sag 或 saga 方法。

3. max_iter：算法收敛的最大迭代次数，默认为 10。

4. n_jobs：拟合和预测的时候 CPU 的核数，默认是 1，也可以是整数，如果是 -1 则代表 CPU 的核数。

   当我们创建好之后，就可以使用 fit 函数拟合，使用 predict 函数预测。

**模型评估指标**

之前对模型做评估时，通常采用的是准确率 (accuracy)，它指的是分类器正确分类的样本数与总体样本数之间的比例。这个指标对大部分的分类情况是有效的，不过当分类结果严重不平衡的时候，准确率很难反应模型的好坏。

举个例子，对于机场安检中恐怖分子的判断，就不能采用准确率对模型进行评估。我们知道恐怖分子的比例是极低的，因此当我们用准确率做判断时，如果准确率高达 99.999%，就说明这个模型一定好么？其实正因为现实生活中恐怖分子的比例极低，就算我们不能识别出一个恐怖分子，也会得到非常高的准确率。因为准确率的评判标准是正确分类的样本个数与总样本数之间的比例。因此非恐怖分子的比例会很高，就算我们识别不出来恐怖分子，正确分类的个数占总样本的比例也会很高，也就是准确率高。

实际上我们应该更关注恐怖分子的识别，这里先介绍下数据预测的四种情况：TP、FP、TN、FN。我们用第二个字母 P 或 N 代表预测为正例还是负例，P 为正，N 为负。第一个字母 T 或 F 代表的是预测结果是否正确，T 为正确，F 为错误。

所以四种情况分别为：TP：预测为正，判断正确；FP：预测为正，判断错误；TN：预测为负，判断正确；FN：预测为负，判断错误。

我们知道样本总数 =TP+FP+TN+FN，预测正确的样本数为 TP+TN，因此准确率 Accuracy = (TP+TN)/(TP+TN+FN+FP)。

实际上，对于分类不平衡的情况，有两个指标非常重要，它们分别是精确度和召回率。

精确率 P = TP/ (TP+FP)，对应上面恐怖分子这个例子，在所有判断为恐怖分子的人数中，真正是恐怖分子的比例。

召回率 R = TP/ (TP+FN)，也称为查全率。代表的是恐怖分子被正确识别出来的个数与恐怖分子总数的比例。

举个例子，假设我们把机场所有的人都认为是恐怖分子，恐怖分子都会被正确识别，这个数字与恐怖分子的总数比例等于 100%，但是这个结果是没有意义的。如果我们认为机场里所有人都是恐怖分子的话，那么非恐怖分子（极高比例）都会认为是恐怖分子，误判率太高了，所以我们还需要统计精确率作为召回率的补充。

实际上有一个指标综合了精确率和召回率，可以更好地评估模型的好坏。这个指标叫做 F1，用公式表示为：
$$
F1=2*\frac{P*R}{P+R}
$$
F1 作为精确率 P 和召回率 R 的调和平均，数值越大代表模型的结果越好。

**对信用卡违约率进行分析**

这个数据集你可以从百度网盘（因为数据集大于 100M，所以采用了网盘）中下载：链接：https://pan.baidu.com/s/14F8WuX0ZJntdB_r1EC08HA 提取码：58gp，这个数据集在kaggle中也能找到，而且这个数据集是个典型的非平衡样本，处理方法可参照kaggle，或者[Credit Fraud](C:\Users\Lenovo\Desktop\学习\自学\Creadit fraud——Kaggle(非平衡面板数据处理流程).ipynb)。

数据集包括了 2013 年 9 月份两天时间内的信用卡交易数据，284807 笔交易中，一共有 492 笔是欺诈行为。输入数据一共包括了 28 个特征 V1，V2，……V28 对应的取值，以及交易时间 Time 和交易金额 Amount。为了保护数据隐私，我们不知道 V1 到 V28 这些特征代表的具体含义，只知道这 28 个特征值是通过 PCA 变换得到的结果。另外字段 Class 代表该笔交易的分类，Class=0 为正常（非欺诈），Class=1 代表欺诈。

1. 加载数据；

2. 准备阶段：我们需要探索数据，用数据可视化的方式查看分类结果的情况，以及随着时间的变化，欺诈交易和正常交易的分布情况。上面已经提到过，V1-V28 的特征值都经过 PCA 的变换，但是其余的两个字段，Time 和 Amount 还需要进行规范化。Time 字段和交易本身是否为欺诈交易无关，因此我们不作为特征选择，只需要对 Amount 做数据规范化就行了。同时数据集没有专门的测试集，使用 train_test_split 对数据集进行划分；

3. 分类阶段：我们需要创建逻辑回归分类器，然后传入训练集数据进行训练，并传入测试集预测结果，将预测结果与测试集的结果进行比对。这里的模型评估指标用到了精确率、召回率和 F1 值。同时我们将精确率 - 召回率进行了可视化呈现。

   基于上面的流程，具体代码如下：

```python

# -*- coding:utf-8 -*-
# 使用逻辑回归对信用卡欺诈进行分类
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import itertools
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, precision_recall_curve
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')
 
# 混淆矩阵可视化
def plot_confusion_matrix(cm, classes, normalize = False, title = 'Confusion matrix"', cmap = plt.cm.Blues) :
    plt.figure()
    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation = 0)
    plt.yticks(tick_marks, classes)
 
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :
        plt.text(j, i, cm[i, j],
                 horizontalalignment = 'center',
                 color = 'white' if cm[i, j] > thresh else 'black')
 
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()
 
# 显示模型评估结果
def show_metrics():
    tp = cm[1,1]
    fn = cm[1,0]
    fp = cm[0,1]
    tn = cm[0,0]
    print('精确率: {:.3f}'.format(tp/(tp+fp)))
    print('召回率: {:.3f}'.format(tp/(tp+fn)))
    print('F1值: {:.3f}'.format(2*(((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn))))))
# 绘制精确率-召回率曲线
def plot_precision_recall():
    plt.step(recall, precision, color = 'b', alpha = 0.2, where = 'post')
    plt.fill_between(recall, precision, step ='post', alpha = 0.2, color = 'b')
    plt.plot(recall, precision, linewidth=2)
    plt.xlim([0.0,1])
    plt.ylim([0.0,1.05])
    plt.xlabel('召回率')
    plt.ylabel('精确率')
    plt.title('精确率-召回率 曲线')
    plt.show();
 
# 数据加载
data = pd.read_csv('./creditcard.csv')
# 数据探索
print(data.describe())
# 设置plt正确显示中文
plt.rcParams['font.sans-serif'] = ['SimHei']
# 绘制类别分布
plt.figure()
ax = sns.countplot(x = 'Class', data = data)
plt.title('类别分布')
plt.show()
# 显示交易笔数，欺诈交易笔数
num = len(data)
num_fraud = len(data[data['Class']==1]) 
print('总交易笔数: ', num)
print('诈骗交易笔数：', num_fraud)
print('诈骗交易比例：{:.6f}'.format(num_fraud/num))
# 欺诈和正常交易可视化
f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,8))
bins = 50
ax1.hist(data.Time[data.Class == 1], bins = bins, color = 'deeppink')
ax1.set_title('诈骗交易')
ax2.hist(data.Time[data.Class == 0], bins = bins, color = 'deepskyblue')
ax2.set_title('正常交易')
plt.xlabel('时间')
plt.ylabel('交易次数')
plt.show()
# 对Amount进行数据规范化
data['Amount_Norm'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))
# 特征选择
y = np.array(data.Class.tolist())
data = data.drop(['Time','Amount','Class'],axis=1)
X = np.array(data.as_matrix())
# 准备训练集和测试集
train_x, test_x, train_y, test_y = train_test_split (X, y, test_size = 0.1, random_state = 33)
 
# 逻辑回归分类
clf = LogisticRegression()
clf.fit(train_x, train_y)
predict_y = clf.predict(test_x)
# 预测样本的置信分数
score_y = clf.decision_function(test_x)  
# 计算混淆矩阵，并显示
cm = confusion_matrix(test_y, predict_y)
class_names = [0,1]
# 显示混淆矩阵
plot_confusion_matrix(cm, classes = class_names, title = '逻辑回归 混淆矩阵')
# 显示模型评估分数
show_metrics()
# 计算精确率，召回率，阈值用于可视化
precision, recall, thresholds = precision_recall_curve(test_y, score_y)
plot_precision_recall()

```

模块说明：

plot_confusion_matrix 函数对混淆矩阵进行可视化。什么是混淆矩阵呢？混淆矩阵也叫误差矩阵，实际上它就是 TP、FP、TN、FN 这四个数值的矩阵表示，帮助我们判断预测值和实际值相比，对了多少。从这个例子中，你能看出 TP=37，FP=7，FN=23。所以精确率 P=TP/(TP+FP)=37/(37+7)=0.841，召回率 R=TP/(TP+FN)=37/(37+23)=0.617。

然后使用了 sklearn 中的 precision_recall_curve 函数，通过预测值和真实值来计算精确率 - 召回率曲线。precision_recall_curve 函数会计算在不同概率阈值情况下的精确率和召回率。最后定义 plot_precision_recall 函数，绘制曲线。

![img](https://static001.geekbang.org/resource/image/ab/50/abee1a58b99814f1e0218778b98a6950.png)



## **41 数据挖掘实战（3）：对比特币进行趋势预测**

数据挖掘算法还有一种叫时间序列分析的算法，时间序列分析模型建立了观察结果与时间变化的关系，能帮我们预测未来一段时间内的结果变化情况。

那么时间序列分析和回归分析有哪些区别呢？

首先，在选择模型前，我们需要确定结果与变量之间的关系。回归分析训练得到的是目标变量 y 与自变量 x（一个或多个）的相关性，然后通过新的自变量 x 来预测目标变量 y。而时间序列分析得到的是目标变量 y 与时间的相关性。

另外，回归分析擅长的是多变量与目标结果之间的分析，即便是单一变量，也往往与时间无关。而时间序列分析建立在时间变化的基础上，它会分析目标变量的趋势、周期、时期和不稳定因素等。这些趋势和周期都是在时间维度的基础上，我们要观察的重要特征。

**时间序列预测**

在时间序列预测模型中，有一些经典的模型，包括 AR、MA、ARMA、ARIMA。

AR 的英文全称叫做 Auto Regressive，中文叫自回归模型。这个算法的思想比较简单，它认为过去若干时刻的点通过线性组合，再加上白噪声就可以预测未来某个时刻的点。在我们日常生活环境中就存在白噪声，在数据挖掘的过程中，你可以把它理解为一个期望为 0，方差为常数的纯随机过程。AR 模型还存在一个阶数，称为 AR（p）模型，也叫作 p 阶自回归模型。它指的是通过这个时刻点的前 p 个点，通过线性组合再加上白噪声来预测当前时刻点的值。

MA 的英文全称叫做 Moving Average，中文叫做滑动平均模型。它与 AR 模型大同小异，AR 模型是历史时序值的线性组合，MA 是通过历史白噪声进行线性组合来影响当前时刻点。AR 模型中的历史白噪声是通过影响历史时序值，从而间接影响到当前时刻点的预测值。同样 MA 模型也存在一个阶数，称为 MA(q) 模型，也叫作 q 阶移动平均模型。我们能看到 AR 和 MA 模型都存在阶数，在 AR 模型中，我们用 p 表示，在 MA 模型中我们用 q 表示，这两个模型大同小异，与 AR 模型不同的是 MA 模型是历史白噪声的线性组合。

ARMA 的英文全称是 Auto Regressive Moving Average，中文叫做自回归滑动平均模型，也就是 AR 模型和 MA 模型的混合。相比 AR 模型和 MA 模型，它有更准确的估计。同样 ARMA 模型存在 p 和 q 两个阶数，称为 ARMA(p,q) 模型。ARIMA 的英文全称是 Auto Regressive Integrated Moving Average 模型，中文叫差分自回归滑动平均模型，也叫求合自回归滑动平均模型。相比于 ARMA，ARIMA 多了一个差分的过程，作用是对不平稳数据进行差分平稳，在差分平稳后再进行建模。ARIMA 的原理和 ARMA 模型一样。相比于 ARMA(p,q) 的两个阶数，ARIMA 是一个三元组的阶数 (p,d,q)，称为 ARIMA(p,d,q) 模型。其中 d 是差分阶数。

**ARMA 模型工具**

```python
from statsmodels.tsa.arima_model import ARMA
```

然后通过 ARMA(endog,order,exog=None) 创建 ARMA 类，这里有一些主要的参数简单说明下：

endog：英文是 endogenous variable，代表内生变量，又叫非政策性变量，它是由模型决定的，不被政策左右，可以说是我们想要分析的变量，或者说是我们这次项目中需要用到的变量。

order：代表是 p 和 q 的值，也就是 ARMA 中的阶数。

exog：英文是 exogenous variables，代表外生变量。外生变量和内生变量一样是经济模型中的两个重要变量。相对于内生变量而言，外生变量又称作为政策性变量，在经济机制内受外部因素的影响，不是我们模型要研究的变量。

举个例子，如果我们想要创建 ARMA(7,0) 模型，可以写成：ARMA(data,(7,0))，其中 data 是我们想要观察的变量，(7,0) 代表 (p,q) 的阶数。创建好之后，我们可以通过 fit 函数进行拟合，通过 predict(start, end) 函数进行预测，其中 start 为预测的起始时间，end 为预测的终止时间。

下面我们使用 ARMA 模型对一组时间序列做建模，代码如下：

```python
# coding:utf-8
# 用ARMA进行时间序列预测
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.tsa.arima_model import ARMA
from statsmodels.graphics.api import qqplot
# 创建数据
data = [5922, 5308, 5546, 5975, 2704, 1767, 4111, 5542, 4726, 5866, 6183, 3199, 1471, 1325, 6618, 6644, 5337, 7064, 2912, 1456, 4705, 4579, 4990, 4331, 4481, 1813, 1258, 4383, 5451, 5169, 5362, 6259, 3743, 2268, 5397, 5821, 6115, 6631, 6474, 4134, 2728, 5753, 7130, 7860, 6991, 7499, 5301, 2808, 6755, 6658, 7644, 6472, 8680, 6366, 5252, 8223, 8181, 10548, 11823, 14640, 9873, 6613, 14415, 13204, 14982, 9690, 10693, 8276, 4519, 7865, 8137, 10022, 7646, 8749, 5246, 4736, 9705, 7501, 9587, 10078, 9732, 6986, 4385, 8451, 9815, 10894, 10287, 9666, 6072, 5418]
data=pd.Series(data)
data_index = sm.tsa.datetools.dates_from_range('1901','1990')
# 绘制数据图
data.index = pd.Index(data_index)
data.plot(figsize=(12,8))
plt.show()
# 创建ARMA模型# 创建ARMA模型
arma = ARMA(data,(7,0)).fit()
print('AIC: %0.4lf' %arma.aic)
# 模型预测
predict_y = arma.predict('1990', '2000')
# 预测结果绘制
fig, ax = plt.subplots(figsize=(12, 8))
ax = data.loc['1901':].plot(ax=ax)
predict_y.plot(ax=ax)
plt.show()
```

怎么判断一个模型是否适合？

我们需要引入 AIC 准则，也叫作赤池消息准则，它是衡量统计模型拟合好坏的一个标准，数值越小代表模型拟合得越好。

**对比特币走势进行预测**

比特币历史数据（从 2012-01-01 到 2018-10-31）可以从 GitHub 上下载：https://github.com/cystanford/bitcoin。

![img](https://static001.geekbang.org/resource/image/b0/36/b0db4047723ec5e649240e2a87196a36.png?wh=468*288)

在准备阶段，我们需要先探索数据，采用数据可视化方式查看比特币的历史走势。按照不同的时间尺度（天，月，季度，年）可以将数据压缩，得到不同尺度的数据，然后做可视化呈现。这 4 个时间尺度上，我们选择月作为预测模型的时间尺度，相应的，我们选择 Weighted_Price 这个字段的数值作为观察结果，在原始数据中，Weighted_Price 对应的是比特币每天的平均价格，当我们以“月”为单位进行压缩的时候，对应的 Weighted_Price 得到的就是当月的比特币平均价格。压缩代码如下：

```python
df_month = df.resample('M').mean()
```

最后在预测阶段创建 ARMA 时间序列模型。我们并不知道 p 和 q 取什么值时，模型最优，因此我们可以给它们设置一个区间范围，比如都是 range(0,3)，然后计算不同模型的 AIC 数值，选择最小的 AIC 数值对应的那个 ARMA 模型。最后用这个最优的 ARMA 模型预测未来 8 个月的比特币平均价格走势，并将结果做可视化呈现。基于这个流程，具体代码如下：

```python

# -*- coding: utf-8 -*-
# 比特币走势预测，使用时间序列ARMA
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima_model import ARMA
import warnings
from itertools import product
from datetime import datetime
warnings.filterwarnings('ignore')
# 数据加载
df = pd.read_csv('./bitcoin_2012-01-01_to_2018-10-31.csv')
# 将时间作为df的索引
df.Timestamp = pd.to_datetime(df.Timestamp)
df.index = df.Timestamp
# 数据探索
print(df.head())
# 按照月，季度，年来统计
df_month = df.resample('M').mean()
df_Q = df.resample('Q-DEC').mean()
df_year = df.resample('A-DEC').mean()
# 按照天，月，季度，年来显示比特币的走势
fig = plt.figure(figsize=[15, 7])
plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签
plt.suptitle('比特币金额（美金）', fontsize=20)
plt.subplot(221)
plt.plot(df.Weighted_Price, '-', label='按天')
plt.legend()
plt.subplot(222)
plt.plot(df_month.Weighted_Price, '-', label='按月')
plt.legend()
plt.subplot(223)
plt.plot(df_Q.Weighted_Price, '-', label='按季度')
plt.legend()
plt.subplot(224)
plt.plot(df_year.Weighted_Price, '-', label='按年')
plt.legend()
plt.show()
# 设置参数范围
ps = range(0, 3)
qs = range(0, 3)
parameters = product(ps, qs)
parameters_list = list(parameters)
# 寻找最优ARMA模型参数，即best_aic最小
results = []
best_aic = float("inf") # 正无穷
for param in parameters_list:
    try:
        model = ARMA(df_month.Weighted_Price,order=(param[0], param[1])).fit()
    except ValueError:
        print('参数错误:', param)
        continue
    aic = model.aic
    if aic < best_aic:
        best_model = model
        best_aic = aic
        best_param = param
    results.append([param, model.aic])
# 输出最优模型
result_table = pd.DataFrame(results)
result_table.columns = ['parameters', 'aic']
print('最优模型: ', best_model.summary())
# 比特币预测
df_month2 = df_month[['Weighted_Price']]
date_list = [datetime(2018, 11, 30), datetime(2018, 12, 31), datetime(2019, 1, 31), datetime(2019, 2, 28), datetime(2019, 3, 31), 
             datetime(2019, 4, 30), datetime(2019, 5, 31), datetime(2019, 6, 30)]
future = pd.DataFrame(index=date_list, columns= df_month.columns)
df_month2 = pd.concat([df_month2, future])
df_month2['forecast'] = best_model.predict(start=0, end=91)
# 比特币预测结果显示
plt.figure(figsize=(20,7))
df_month2.Weighted_Price.plot(label='实际金额')
df_month2.forecast.plot(color='r', ls='--', label='预测金额')
plt.legend()
plt.title('比特币金额（月）')
plt.xlabel('时间')
plt.ylabel('美金')
plt.show()
```

![img](https://static001.geekbang.org/resource/image/24/94/24f8ee2f600a2451eecd58a98f7db894.png?wh=1726*846)



## 42 深度学习

上述算法都属于传统的机器学习算法。那么机器学习算法和深度学习有什么关联呢？

在这篇文章中，我们会通过以下几个方面了解深度学习：

1. 数据挖掘、机器学习和深度学习的区别是什么？这些概念都代表什么？
2. 我们通过深度学习让机器具备人的能力，甚至某些技能的水平超过人类，比如图像识别、下棋对弈等。那么深度学习的大脑是如何工作的？
3. 深度学习是基于神经网络构建的，都有哪些常用的网络模型？
4. 深度学习有三个重要的应用领域，这三个应用领域分别是什么？

**数据挖掘，机器学习，深度学习的区别**

实际上数据挖掘和机器学习在很大程度上是重叠的。一些常用算法，比如 K-Means、KNN、SVM、决策树和朴素贝叶斯等，既可以说是数据挖掘算法，又可以说是机器学习算法。那么数据挖掘和机器学习之间有什么区别呢？

数据挖掘通常是从现有的数据中提取规律模式（pattern）以及使用算法模型（model）。核心目的是找到这些数据变量之间的关系，因此我们也会通过数据可视化对变量之间的关系进行呈现，用算法模型挖掘变量之间的关联关系。通常情况下，我们只能判断出来变量 A 和变量 B 是有关系的，但并不一定清楚这两者之间有什么具体关系。在我们谈论数据挖掘的时候，更强调的是从数据中挖掘价值。

机器学习是人工智能的一部分，它指的是通过训练数据和算法模型让机器具有一定的智能。一般是通过已有的数据来学习知识，并通过各种算法模型形成一定的处理能力，比如分类、聚类、预测、推荐能力等。这样当有新的数据进来时，就可以通过训练好的模型对这些数据进行预测，也就是通过机器的智能帮我们完成某些特定的任务。

深度学习属于机器学习的一种，它的目标同样是让机器具有智能，只是与传统的机器学习算法不同，它是通过神经网络来实现的。神经网络就好比是机器的大脑，刚开始就像一个婴儿一样，是一张白纸。但通过多次训练之后，“大脑”就可以逐渐具备某种能力。这个训练过程中，我们只需要告诉这个大脑输入数据是什么，以及对应的输出结果是什么即可。通过多次训练，“大脑”中的多层神经网络的参数就会自动优化，从而得到一个适应于训练数据的模型。

所以你能看到在传统的机器学习模型中，我们都会讲解模型的算法原理，比如 K-Means 的算法原理，KNN 的原理等。而到了神经网络，我们更关注的是网络结构，以及网络结构中每层神经元的传输机制。我们不需要告诉机器具体的特征规律是什么，只需把我们想要训练的数据和对应的结果告诉机器大脑即可。深度学习会自己找到数据的特征规律！而传统机器学习往往需要专家（我们）来告诉机器采用什么样的模型算法，这就是深度学习与传统机器学习最大的区别。另外深度学习的神经网络结构通常比较深，一般都是 5 层以上，甚至也有 101 层或更多的层数。这些深度的神经网络可以让机器更好地自动捕获数据的特征。

**神经网络是如何工作的**

神经网络可以说是机器的大脑，经典的神经网络结构可以用下面的图来表示。

![img](https://static001.geekbang.org/resource/image/06/b7/06d709117b776add64d1415ae209c0b7.jpg)

**节点：**神经网络是由神经元组成的，也称之为节点，它们分布在神经网络的各个层中，这些层包括输入层，输出层和隐藏层。

**输入层：**负责接收信号，并分发到隐藏层。一般我们将数据传给输入层。

**输出层：**负责输出计算结果，一般来说输出层节点数等于我们要分类的个数。

**隐藏层：**除了输入层和输出层外的神经网络都属于隐藏层，隐藏层可以是一层也可以是多层，每个隐藏层都会把前一层节点传输出来的数据进行计算（你可以理解是某种抽象表示），这相当于把数据抽象到另一个维度的空间中，可以更好地提取和计算数据的特征。

**工作原理：**神经网络就好比一个黑盒子，我们只需要告诉这个黑盒子输入数据和输出数据，神经网络就可以自我训练。一旦训练好之后，就可以像黑盒子一样使用，当你传入一个新的数据时，它就会告诉你对应的输出结果。在训练过程中，神经网络主要是通过前向传播和反向传播机制运作的。什么是前向传播和反向传播呢？

**前向传播：**数据从输入层传递到输出层的过程叫做前向传播。这个过程的计算结果通常是通过上一层的神经元的输出经过矩阵运算和激活函数得到的。这样就完成了每层之间的神经元数据的传输。

**反向传播：**当前向传播作用到输出层得到分类结果之后，我们需要与实际值进行比对，从而得到误差。反向传播也叫作误差反向传播，核心原理是通过代价函数对网络中的参数进行修正，这样更容易让网络参数得到收敛。

所以，整个神经网络训练的过程就是不断地通过前向 - 反向传播迭代完成的，当达到指定的迭代次数或者达到收敛标准的时候即可以停止训练。然后我们就可以拿训练好的网络模型对新的数据进行预测。

当然，深度神经网络是基于神经网络发展起来的，它的原理与神经网络的原理一样，只不过强调了模型结构的深度，通常有 5 层以上，这样模型的学习能力会更强大。

**常用的神经网络**

按照中间层功能的不同，神经网络可以分为三种网络结构，分别为 FNN、CNN 和 RNN。

**FNN**（Fully-connected Neural Network）指的是全连接神经网络，全连接的意思是每一层的神经元与上一层的所有神经元都是连接的。不过在实际使用中，全连接的参数会过多，导致计算量过大。因此在实际使用中全连接神经网络的层数一般比较少。

![img](https://static001.geekbang.org/resource/image/c2/7a/c2bfd532360e5cc026aa12f45c86957a.jpg)

**CNN** 叫作卷积神经网络，在图像处理中有广泛的应用，了解图像识别的同学对这个词一定不陌生。CNN 网络中，包括了卷积层、池化层和全连接层。

卷积层相当于一个滤镜的作用，它可以把图像进行分块，对每一块的图像进行变换操作。

池化层相当于对神经元的数据进行降维处理，这样输出的维数就会减少很多，从而降低整体的计算量。

全连接层通常是输出层的上一层，它将上一层神经元输出的数据转变成一维的向量。

**RNN** 称为循环神经网络，它的特点是神经元的输出可以在下一个时刻作用到自身，这样 RNN 就可以看做是在时间上传递的神经网络。它可以应用在语音识别、自然语言处理等与上下文相关的场景。

深度学习网络往往包括了这三种网络的变种形成，常用的深度神经网络包括 AlexNet、VGG19、GoogleNet、ResNet 等

![img](https://static001.geekbang.org/resource/image/49/9f/490ced74dc6cf70403e73c05f302449f.png)

随着时间的推进，提出的深度学习网络层数越来越深，Top-5 错误率越来越低。

你可能会问什么是 Top-5 错误率，实际上这些网络结构的提出和一个比赛相关，这个比赛叫做 ILSVRC，英文全称叫做 Large Scale Visual Recognition Challenge。它是一个关于大规模图像可视化识别的比赛，所基于的数据集就是著名的 ImageNet 数据集，一共包括了 1400 万张图片，涵盖 2 万多个类别。

表格中的 AlexNet 就是 2012 年的 ILSVRC 冠军，当时的 Top-5 正确率是 84.7%，VGG 和 GoogleNet 是 2014 年 ILSVRC 比赛的模型，其中 GoogleNet 是当时比赛的冠军，而 VGG 是当时比赛的亚军，它的效率低于 GoogleNet。VGG 有两个版本，VGG16 和 VGG19，分别是 16 层和 19 层的 VGG 网络，这两者没有本质的区别，只是网络深度不同。到了 2015 年，比赛冠军是 ResNet，Top-5 正确率达到了 96.43%。ResNet 也有不同的版本，比如 ResNet50、ResNet101 和 ResNet152 等，名称后面的数字代表的是不同的网络深度。之后 ResNet 在其他图像比赛中也多次拿到冠军。

**深度学习的应用领域**

从 ImageNet 跑出来的这些优秀模型都是基于 CNN 卷积神经网络的。实际上深度学习有三大应用领域，图像识别就是其中之一，其他领域分别是语音识别和自然语言处理。这三个应用领域有一个共同的特性，就是都来自于信号处理。

我们人类平时会处理图像信息，语音信息以及语言文字信息。机器可以帮助我们完成这三个应用里的某些工作。比如图像识别领域中图像分类和物体检测就是两个核心的任务。我们可以让机器判断图像中都有哪些物体，类别是什么，以及这些物体所处的位置。图像识别被广泛应用在安防检测中。此外人脸识别也是图像识别重要的应用场景。

Siri 大家一定不陌生，此外还有我们使用的智能电视等，都采用了语音识别技术。语音识别技术可以识别人类的语音指令并进行交互。在语音导航中，还采用了语音合成技术，这样就可以让机器模拟人的声音为我们服务，Siri 语音助手也采用了语音识别和合成的技术。自然语言处理的英文缩写是 NLP，它被广泛应用到自动问答、智能客服、过滤垃圾邮件和短信等领域中。在电商领域，我们可以通过 NLP 自动给商品评论打标签，在用户决策的时候提供数据支持。在自动问答中，我们可以输入自己想问的问题，让机器来回答，比如在百度中输入“姚明的老婆”，就会自动显示出”叶莉“。

![img](https://static001.geekbang.org/resource/image/e8/b9/e88beadebefd5789efb08284193f65b9.png)



## 43 用Keras搭建深度学习网络做手写数字识别

学习目标主要有以下的几个方面：

1. 进一步了解 CNN 网络。CNN 网络在深度学习网络中应用很广，很多网络都是基于 CNN 网络构建的，你有必要进一步了解 CNN 的网络层次，尤其是关于卷积的原理。
2. 初步了解 LeNet 和 AlexNet。它们都是经典的 CNN 网络，我们今天的任务就是认识这些经典的 CNN 网络，这样在接触更深度的 CNN 网络的时候，比如 VGG、GoogleNet 和 ResNet 这些网络的时候，就会更容易理解和使用。
3. 对常用的深度学习框架进行对比，包括 Tensorflow、Keras、Caffe、PyTorch、 MXnet 和 Theano。当选择深度学习框架的时候到底该选择哪个？
4. 使用 Keras 这个深度学习框架编写代码，完成第一个深度学习任务，也就是 Mnist 手写数字识别。

**CNN 网络中的卷积作用**

CNN 的网络结构由三种层组成，它们分别是卷积层、池化层和全连接层。

在上文中，我讲到卷积层相当于滤镜的作用，它可以把图像分块，对每一块的图像进行卷积操作。卷积本身是一种矩阵运算，那什么是卷积呢？

假设我有一个二维的图像 X，和卷积 K，把二维矩阵 X 进行卷积 K 操作之后，可以得到矩阵 Z，如下图所示：

![img](https://static001.geekbang.org/resource/image/9d/cf/9d1bb65b30517775b632c10c1cb1c0cf.jpg)

第一步，我们需要将卷积核翻转 180 度（只有翻转之后才能做矩阵运算），也就是变成：

![img](https://static001.geekbang.org/resource/image/cb/33/cb755c0cf5868c39d71e0392146c4833.jpg)

第二步，将卷积核的第一个元素，对准矩阵 X 左上角的第一个元素，对应元素相乘，然后再相加可以就可以得到 10*1+10*1+10*0+10*1+5*0+5*-1+10*0+5*-1+5*-1=15。

![img](https://static001.geekbang.org/resource/image/90/11/90a3bbabba732a2a7ad97a24f3587411.jpg)

第三步，每个元素都重复第二步的计算过程，可以得到如下的矩阵结果 Z：

![img](https://static001.geekbang.org/resource/image/b8/6c/b824778383e3a898fe2399fb2eb8846c.jpg)

这样我们就完成了一个卷积的操作。代码如下：

```python

import pylab
import numpy as np
from scipy import signal
# 设置原图像
img = np.array([[10, 10, 10, 10, 10],
                     [10, 5, 5, 5, 10],
                     [10, 5, 5, 5, 10],
                     [10, 5, 5, 5, 10],
                     [10, 10, 10, 10, 10]])
# 设置卷积核
fil = np.array([[ -1,-1, 0],
                [ -1, 0, 1],
                [  0, 1, 1]])
# 对原图像进行卷积操作
res = signal.convolve2d(img, fil, mode='valid')
# 输出卷积后的结果
print(res)
```

这里我用到了 convolve2d 函数对图像 img 和卷积核 fil 做卷积运算，最后输出结果 res。你可能还是会问，为什么我们要对图像进行卷积操作呢？你可以看看下面一段代码：

```python

import matplotlib.pyplot as plt
import pylab
import cv2
import numpy as np
from scipy import signal
# 读取灰度图像
img = cv2.imread("haibao.jpg", 0)
# 显示灰度图像
plt.imshow(img,cmap="gray")
pylab.show()
# 设置卷积核
fil = np.array([[ -1,-1, 0],
                [ -1, 0, 1],
                [  0, 1, 1]])
# 卷积操作
res = signal.convolve2d(img, fil, mode='valid')
print(res)
#显示卷积后的图片
plt.imshow(res,cmap="gray")
pylab.show()
```

这里我对专栏的海报做了卷积的操作，你能看到卷积操作是对图像进行了特征的提取。实际上每个卷积核都是一种滤波器，它们把图像中符合条件的部分筛选出来，也就相当于做了某种特征提取。

在 CNN 的卷积层中可以有多个卷积核，以 LeNet 为例，它的第一层卷积核有 6 个，因此可以帮我们提取出图像的 6 个特征，从而得到 6 个特征图（feature maps）。

**激活函数的作用**

做完卷积操作之后，通常还需要使用激活函数对图像进一步处理。在逻辑回归中，我提到过 Sigmoid 函数，它在深度学习中有广泛的应用，除了 Sigmoid 函数作为激活函数以外，tanh、ReLU 都是常用的激活函数。

这些激活函数通常都是非线性的函数，使用它们的目的是把线性数值映射到非线性空间中。卷积操作实际上是两个矩阵之间的乘法，得到的结果也是线性的。只有经过非线性的激活函数运算之后，才能映射到非线性空间中，这样也可以让神经网络的表达能力更强大。

**池化层的作用**

池化层通常在两个卷积层之间，它的作用相当于对神经元的数据做降维处理，这样就能降低整体计算量。

假设池化的窗大小是 2x2，就相当于用一个 2x2 的窗口对输出数据进行计算，将原图中 2x2 矩阵的 4 个点变成一个点。常用的池化操作是平均池化和最大池化。平均池化是对特征点求平均值，也就是用 4 个点的平均值来做代表。最大池化则是对特征点求最大值，也就是用 4 个点的最大值来做代表。

在神经网络中，我们可以叠加多个卷积层和池化层来提取更抽象的特征。经过几次卷积和池化之后，通常会有一个或多个全连接层。

**全连接层的作用**

全连接层将前面一层的输出结果与当前层的每个神经元都进行了连接。

这样就可以把前面计算出来的所有特征，通过全连接层将输出值输送给分类器，比如 Softmax 分类器。在深度学习中，Softmax 是个很有用的分类器，通过它可以把输入值映射到 0-1 之间，而且所有输出结果相加等于 1。其实你可以换种方式理解这个概念，假设我们想要识别一个数字，从 0 到 9 都有可能。那么通过 Softmax 层，对应输出 10 种分类结果，每个结果都有一个概率值，这些概率相加为 1，我们就可以知道这个数字是 0 的概率是多少，是 1 的概率是多少……是 9 的概率又是多少，从而也就帮我们完成了数字识别的任务。

**LeNet 和 AlexNet 网络**

你能看出 CNN 网络结构中每一层的作用：它通过卷积层提取特征，通过激活函数让结果映射到非线性空间，增强了结果的表达能力，再通过池化层压缩特征图，降低了网络复杂度，最后通过全连接层归一化，然后连接 Softmax 分类器进行计算每个类别的概率。

通常我们可以使用多个卷积层和池化层，最后再连接一个或者多个全连接层，这样也就产生了不同的网络结构，比如 LeNet 和 AlexNet。

我将 LeNet 和 AlexNet 的参数特征整理如下：

![img](https://static001.geekbang.org/resource/image/dd/d1/dd0dbbcd6797bf9560c306025ee6fbd1.png)

LeNet 提出于 1986 年，是最早用于数字识别的 CNN 网络，输入尺寸是 32*32。它输入的是灰度的图像，整个的网络结构是：输入层→C1 卷积层→S2 池化层→C3 卷积层→S4 池化层→C5 卷积层→F6 全连接层→Output 全连接层，对应的 Output 输出类别数为 10。*

*AlexNet 在 LeNet 的基础上做了改进，提出了更深的 CNN 网络模型，输入尺寸是 227*227*3，可以输入 RGB 三通道的图像，整个网络的结构是：输入层→(C1 卷积层→池化层)→(C2 卷积层→池化层)→C3 卷积层→C4 卷积层→(C5 池化层→池化层)→全连接层→全连接层→Output 全连接层。

实际上后面提出来的深度模型，比如 VGG、GoogleNet 和 ResNet 都是基于下面的这种结构方式改进的：输出层→（卷积层 + -> 池化层？）+ → 全连接层 +→Output 全连接层。其中“+”代表 1 个或多个，“？”代表 0 个或 1 个。

你能看出卷积层后面可以有一个池化层，也可以没有池化层，“卷积层 + → 池化层？”这样的结构算是一组卷积层，在多组卷积层之后，可以连接多个全连接层，最后再接 Output 全连接层。

**常用的深度学习框架对比**

![img](https://static001.geekbang.org/resource/image/ea/67/ea523df67c73d19732df1d172b30fd67.png)

从 GitHub 上的热门程序排序来看，Tensorflow、Keras 和 Caffe 是三个排名最高的深度学习框架，其中 Tensorflow 是 Google 出品，也是深度学习最常用的库。关于 Keras，你可以理解成是把 Tensorflow 或 Theano 作为后端，基于它们提供的封装接口，这样更方便我们操作使用。Caffe、PyTorch、MXNet 和 Theano 也是常用的深度学习库，你在接触深度学习的时候可能也会遇到，这里不做介绍。



**用 Keras 做 Mnist 手写数字识别**

```python

pip install keras
pip install tensorflow
#这里需要注明的是 Keras 需要用 tensorflow 或者 theano 作为后端，因此我们也需要引入相关的工具。同时你还需要注意 NumPy 版本是否为最新的版本，我们需要采用#最新的 NumPy 版本才能正常运行 keras，更新 NumPy 工具的方法：
pip install -U numpy
#安装好 Keras 工具包之后，就可以创建一个 Sequential 序贯模型，它的作用是将多个网络层线性堆叠起来，使用方法：

from keras.models import Sequential
model = Sequential()
```

**创建二维卷积层**

使用 Conv2D(filters, kernel_size, activation=None) 进行创建, 其中 filters 代表卷积核的数量，kernel_size 代表卷积核的宽度和长度，activation 代表激活函数。如果创建的二维卷积层是第一个卷积层，我们还需要提供 input_shape 参数，比如：input_shape=(28, 28, 1) 代表的就是 28*28 的灰度图像。

**对 2D 信号做最大池化层**

使用 MaxPooling2D(pool_size=(2, 2)) 进行创建，其中 pool_size 代表下采样因子，比如 pool_size=(2,2) 的时候相当于将原来 22 的矩阵变成一个点，即用 22 矩阵中的最大值代替，输出的图像在长度和宽度上均为原图的一半。

**创建 Flatten 层**

使用 Flatten() 创建，常用于将多维的输入扁平化，也就是展开为一维的向量。一般用在卷积层与全连接层之间，方便后面进行全连接层的操作。

**创建全连接层**

使用 Dense(units, activation=None) 进行创建，其中 units 代表的是输出的空间维度，activation 代表的激活函数。

我这里只列举了部分常用的层，这些层在今天手写数字识别的项目中会用到。当我们把层创建好之后，可以加入到模型中，使用 model.add() 函数即可。

添加好网络模型中的层之后，我们可以使用 model.compile(loss, optimizer=‘adam’, metrics=[‘accuracy’]) 来完成损失函数和优化器的配置，其中 loss 代表损失函数的配置，optimizer 代表优化器，metrics 代表评估模型所采用的指标。

然后我们可以使用 fit 函数进行训练，使用 predict 函数进行预测，使用 evaluate 函数对模型评估。

针对 Mnist 手写数字识别，用 keras 的实现代码如下：

```python

# 使用LeNet模型对Mnist手写数字进行识别
import keras
from keras.datasets import mnist
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Dense, Flatten
from keras.models import Sequential
# 数据加载
(train_x, train_y), (test_x, test_y) = mnist.load_data()
# 输入数据为 mnist 数据集
train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)
test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)
train_x = train_x / 255
test_x = test_x / 255
train_y = keras.utils.to_categorical(train_y, 10)
test_y = keras.utils.to_categorical(test_y, 10)
# 创建序贯模型
model = Sequential()
# 第一层卷积层：6个卷积核，大小为5∗5, relu激活函数
model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))
# 第二层池化层：最大池化
model.add(MaxPooling2D(pool_size=(2, 2)))
# 第三层卷积层：16个卷积核，大小为5*5，relu激活函数
model.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))
# 第二层池化层：最大池化
model.add(MaxPooling2D(pool_size=(2, 2)))
# 将参数进行扁平化，在LeNet5中称之为卷积层，实际上这一层是一维向量，和全连接层一样
model.add(Flatten())
model.add(Dense(120, activation='relu'))
# 全连接层，输出节点个数为84个
model.add(Dense(84, activation='relu'))
# 输出层 用softmax 激活函数计算分类概率
model.add(Dense(10, activation='softmax'))
# 设置损失函数和优化器配置
model.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])
# 传入训练数据进行训练
model.fit(train_x, train_y, batch_size=128, epochs=2, verbose=1, validation_data=(test_x, test_y))
# 对结果进行评估
score = model.evaluate(test_x, test_y)
print('误差:%0.4lf' %score[0])
print('准确率:', score[1])
```

**总结**

今天我们用 keras 对手写数字进行了识别，具体的代码部分讲解的不多，其中涉及到 API，你可以参考下 Keras 中文手册。

在这个过程里，我们只是使用了 LeNet 的网络模型，实际上 AlexNet、VGG、GoogleNet 和 ResNet 都是基于 CNN 的网络结构。在 CNN 网络中包括了卷积层、池化层和全连接层。一个基于 CNN 的深度学习网络通常是几组卷积层之后，再连接多个全连接层，最后再接 Output 全连接层，而每组的卷积层都是“卷积层 + →池化层？”的结构。

另外，通过今天的学习你应该能体会到卷积在图像领域中的应用。今天我对专栏的海报进行了一个 3*3 的卷积核操作，可以看到卷积之后得到的图像是原图像某种特征的提取。在实际的卷积层中，会包括多个卷积核，对原图像在不同特征上进行提取。通过多个卷积层的操作，可以在更高的维度上对图像特征进一步提取，这样可以让机器在不同层次、不同维度理解图像特征。

另外在 Keras 使用中，你能看到与 sklearn 中的机器学习算法使用不同。我们需要对网络模型中的层进行配置，将创建好的层添加到模型中，然后对模型中使用的损失函数和优化器进行配置，最后就可以对它进行训练和预测了。

![img](https://static001.geekbang.org/resource/image/43/39/431ccdf001d421b3810e03c9c598b539.png)

